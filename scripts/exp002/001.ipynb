{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = \"002\"\n",
    "    ver = \"001\"\n",
    "    training = True\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    num_proc = 4\n",
    "\n",
    "    threshold = 0.99\n",
    "\n",
    "    tokenize_options = {\n",
    "        \"return_offsets_mapping\": True,\n",
    "        \"truncation\": False,\n",
    "        \"max_length\": 4096,\n",
    "    }\n",
    "    deberta_options = {\n",
    "        \"output_hidden_states\": True,\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"layer_norm_eps\": 1e-7,\n",
    "        \"add_pooling_layer\": False,\n",
    "    }\n",
    "\n",
    "    model_name = \"deberta3base-truncation-false\"\n",
    "    freeze_layers = 0\n",
    "\n",
    "    save_path = f\"/kaggle/input/{model_name}-{exp}-{ver}\"\n",
    "\n",
    "    output_dir = \"/kaggle/output\"\n",
    "\n",
    "    model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall\"\n",
    "\n",
    "    train_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n",
    "    test_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
    "    moredata_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"\n",
    "    pii_dataset_fixed_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"\n",
    "\n",
    "    batch_size = 1\n",
    "    epochs = 3\n",
    "    lr = 1e-5\n",
    "\n",
    "    all_labels = [\n",
    "        \"B-EMAIL\",\n",
    "        \"B-ID_NUM\",\n",
    "        \"B-NAME_STUDENT\",\n",
    "        \"B-PHONE_NUM\",\n",
    "        \"B-STREET_ADDRESS\",\n",
    "        \"B-URL_PERSONAL\",\n",
    "        \"B-USERNAME\",\n",
    "        \"I-ID_NUM\",\n",
    "        \"I-NAME_STUDENT\",\n",
    "        \"I-PHONE_NUM\",\n",
    "        \"I-STREET_ADDRESS\",\n",
    "        \"I-URL_PERSONAL\",\n",
    "        \"O\",\n",
    "    ]\n",
    "    num_pii_labels = len(all_labels) - 1\n",
    "    label2id = {label: index for index, label in enumerate(all_labels)}\n",
    "    id2label = {index: label for index, label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "sys.path.append('/kaggle/input/piimetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from comp_metric import compute_metrics\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    DebertaV2Config,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "# from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(Path(Config.train_path))\n",
    "    print(f\"kaggle train data = {len(train_data)}\")\n",
    "\n",
    "    more_data = pd.read_json(Path(Config.moredata_path))\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "\n",
    "    pii_dataset_fixed = pd.read_json(Path(Config.pii_dataset_fixed_path))\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    df[\"document\"] = [i for i in range(len(df))]  # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=Config.seed)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_true_samples = len(true_labels) - int(300 * len(true_labels) / len(df))\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_true_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - (300 - int(300 * len(true_labels) / len(df)))\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, is_test: bool) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        if is_test:\n",
    "            self.tokenized_df = df.apply(self.tokenize_test, axis=1, result_type=\"expand\")\n",
    "        else:\n",
    "            self.tokenized_df = df.apply(self.tokenize_train, axis=1, result_type=\"expand\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_df)\n",
    "    \n",
    "    def tokenize_train(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        labels = []\n",
    "        targets = []\n",
    "        idx = 0\n",
    "        for t, l, ws in zip(row[\"tokens\"], row[\"labels\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            labels.extend([l]*len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if l in Config.all_labels:  \n",
    "                targets.append(1)\n",
    "            else:\n",
    "                targets.append(0)\n",
    "            \n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "            idx += 1\n",
    "\n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "         \n",
    "        target_num = sum(targets)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            if start_idx == 0 and end_idx == 0: \n",
    "                token_labels.append(Config.label2id[\"O\"])\n",
    "                continue\n",
    "            \n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            try:\n",
    "                token_labels.append(Config.label2id[labels[start_idx]])\n",
    "            except:\n",
    "                continue\n",
    "        length = len(tokenized.input_ids)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"labels\": token_labels,\n",
    "            \"length\": length,\n",
    "            \"target_num\": target_num,\n",
    "            \"group\": 1 if target_num > 0 else 0,\n",
    "            \"token_map\": token_map,\n",
    "            \"document\": row[\"document\"],\n",
    "            \"tokens\": row[\"tokens\"],      \n",
    "            \"raw_labels\": row[\"labels\"],     \n",
    "        }\n",
    "\n",
    "    def tokenize_test(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        \n",
    "        idx = 0\n",
    "        for t, ws in zip(row[\"tokens\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            token_map.extend([idx]*len(t))\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                token_map.append(-1)\n",
    "                \n",
    "            idx += 1\n",
    "            \n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"token_map\": token_map,       \n",
    "            \"document\": row[\"document\"],\n",
    "            \"tokens\": row[\"tokens\"],         \n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            row = self.tokenized_df.drop([\"document\", \"tokens\"], axis=1).iloc[index]\n",
    "        else:\n",
    "            row = self.tokenized_df.drop([\"document\", \"tokens\", \"raw_labels\"], axis=1).iloc[index]\n",
    "        return row\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    DataFrameからモデリング時に使用するDataModuleを作成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        valid_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = Config.batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collator = DataCollatorForTokenClassification(\n",
    "            tokenizer, pad_to_multiple_of=512\n",
    "        )\n",
    "        self.reference_df = self.create_val_reference_df(valid_df)\n",
    "\n",
    "    def create_val_reference_df(self, valid_df: pd.DataFrame):\n",
    "        valid_df = valid_df[['document', 'tokens', 'labels']].copy()\n",
    "        valid_df = valid_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n",
    "        valid_df['token'] = valid_df.groupby('document').cumcount()\n",
    "        \n",
    "        reference_df = valid_df[valid_df['label'] != 'O'].copy()\n",
    "        reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n",
    "        reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n",
    "        return reference_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CreateDataset(\n",
    "            self.train_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.valid_dataset = CreateDataset(\n",
    "            self.valid_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.test_dataset = CreateDataset(\n",
    "            self.test_df, self.tokenizer, is_test=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dfs():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    # Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "    train_df, valid_df = downsample_df(df.copy())\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    valid_df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Number of train_df = {len(train_df)}\")\n",
    "    print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "\n",
    "    test_df = pd.read_json(Path(Config.test_path))\n",
    "    clear_memory()\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "more data = 2000\n",
      "pii_dataset_fixed = 4434\n",
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7203 true_others = 166\n",
      "false_samples = 5738 false_others = 134\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df, test_df = gen_dfs()\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_path)\n",
    "dm = CreateDataModule(train_df, valid_df, test_df, tokenizer)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_preds(logits_list: list[torch.Tensor], is_train: bool = True) -> list[torch.Tensor]:\n",
    "    preds_final = []\n",
    "    for logits in logits_list:\n",
    "        predictions_prob = torch.softmax(logits, dim=-1)\n",
    "        predictions = logits.argmax(-1)\n",
    "        predictions_without_O = predictions_prob[:, :12].argmax(-1)\n",
    "\n",
    "        O_prob = predictions_prob[:, 12]\n",
    "        pred_final = torch.where(\n",
    "            O_prob < Config.threshold, predictions_without_O, predictions\n",
    "        )\n",
    "        preds_final.append(pred_final)\n",
    "\n",
    "    return preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_df(preds_list: list[torch.Tensor], tokenized_df: pd.DataFrame):\n",
    "    triplets = []\n",
    "    pairs = set()\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for preds, token_map, offsets, tokens, doc in zip(\n",
    "        preds_list,\n",
    "        tokenized_df[\"token_map\"],\n",
    "        tokenized_df[\"offset_mapping\"],\n",
    "        tokenized_df[\"tokens\"],\n",
    "        tokenized_df[\"document\"],\n",
    "    ):\n",
    "        # p = p.argmax(-1).cpu().detach().numpy()\n",
    "        preds = preds.cpu().detach().numpy()\n",
    "\n",
    "        for token_pred, (start_idx, end_idx) in zip(preds, offsets):\n",
    "            label_pred = Config.id2label[(token_pred)]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            if label_pred == \"O\" or token_id == -1:\n",
    "                continue\n",
    "\n",
    "            pair = (doc, token_id)\n",
    "\n",
    "            if pair in pairs:\n",
    "                continue\n",
    "\n",
    "            document.append(doc)\n",
    "            token.append(token_id)\n",
    "            label.append(label_pred)\n",
    "            token_str.append(tokens[token_id])\n",
    "            pairs.add(pair)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": document, \"token\": token, \"label\": label, \"token_str\": token_str}\n",
    "    )\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (_, _) = self.lstm.forward(x)\n",
    "        out = hidden\n",
    "        return out\n",
    "\n",
    "\n",
    "class DebertaLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, dm: CreateDataModule):\n",
    "        super(DebertaLSTMModel, self).__init__()\n",
    "\n",
    "        # self.example_input_array = torch.Tensor(1, 1).to(dtype=torch.int64)\n",
    "        self.dm = dm\n",
    "\n",
    "        self.model_config: DebertaV2Config = AutoConfig.from_pretrained(\n",
    "            Config.model_path\n",
    "        )\n",
    "\n",
    "        self.model_config.update(Config.deberta_options)\n",
    "\n",
    "        self.transformers_model: PreTrainedModel = AutoModel.from_pretrained(\n",
    "            Config.model_path\n",
    "        )\n",
    "        self.head = LSTMHead(\n",
    "            in_features=self.model_config.hidden_size,\n",
    "            hidden_dim=self.model_config.hidden_size // 2,\n",
    "            n_layers=1,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.model_config.hidden_size, len(Config.all_labels))\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='mean',ignore_index=-100) \n",
    "        self.validation_step_outputs = []        \n",
    "\n",
    "        # if Config.freeze_layers>0:\n",
    "        # \tprint(f'Freezing {Config.freeze_layers} layers.')\n",
    "        # \tfor layer in self.transformers_model.longformer.encoder.layer[:Config.freeze_layers]:\n",
    "        # \t\tfor param in layer.parameters():\n",
    "        # \t\t\tparam.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, train=True):\n",
    "        transformer_out: BaseModelOutput = self.transformers_model.forward(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = transformer_out.last_hidden_state\n",
    "        head_output = self.head.forward(last_hidden_state)\n",
    "        # last_hidden_state = (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        logits = self.fc.forward(head_output)\n",
    "\n",
    "        # logits = (batch_size, seq_len, num_labels)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        logits = self.forward(input_ids, attention_mask, train=True)\n",
    "        # output = (seq_len, num_labels)\n",
    "        loss = self.loss_function(\n",
    "            logits.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def train_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        print(f\"epoch {self.trainer.current_epoch} training loss {avg_loss}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids: torch.Tensor = batch[\"input_ids\"]\n",
    "        attention_mask: torch.Tensor = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        logits = self.forward(input_ids, attention_mask, train=False)\n",
    "        # logits.shape = (batch_size, seq_len, num_labels)\n",
    "\n",
    "        loss = self.loss_function(\n",
    "            logits.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.validation_step_outputs.append(\n",
    "            {\"val_loss\": loss, \"logits\": logits, \"targets\": target}\n",
    "        )\n",
    "        return {\"val_loss\": loss, \"logits\": logits, \"targets\": target}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        logits_list = [logits for batch in outputs for logits in batch[\"logits\"]]\n",
    "        preds_list = post_processing_preds(logits_list)\n",
    "        \n",
    "        pred_df = predictions_to_df(preds_list, self.dm.valid_dataset.tokenized_df)\n",
    "\n",
    "        print(pred_df.shape)\n",
    "        print(pred_df)\n",
    "\n",
    "        self.validation_step_outputs = []    \n",
    "\n",
    "        # print(output_val.shape)\n",
    "        labels_gt = [target for batch in outputs for target in batch[\"targets\"]]\n",
    "        labels_pred = pred_df[\"label\"].to_list()\n",
    "        avg_score = compute_metrics(pred_df, self.dm.reference_df)\n",
    "        f5_score = avg_score[\"ents_f5\"]\n",
    "        print(f\"epoch {self.trainer.current_epoch} validation loss {avg_loss}\")\n",
    "        print(f\"epoch {self.trainer.current_epoch} validation scores {avg_score}\")\n",
    "\n",
    "        return {\"val_loss\": avg_loss, \"val_f5\": f5_score}\n",
    "\n",
    "    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"transformers_model\" not in n\n",
    "                ],\n",
    "                \"lr\": decoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=Config.lr)\n",
    "\n",
    "        epoch_steps = len(self.dm.train_dataset)\n",
    "        batch_size = Config.batch_size\n",
    "\n",
    "        warmup_steps = 0.05 * epoch_steps // batch_size\n",
    "        training_steps = Config.epochs * epoch_steps // batch_size\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n",
    "        # scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=1e-6, power=3.0)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, warmup_steps, training_steps, num_cycles=0.5\n",
    "        )\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a0aa8baa274f38bc8daea005045d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1261, 5)\n",
      "      document  token        label token_str  row_id\n",
      "0           34      0  I-PHONE_NUM    Design       0\n",
      "1           34      1  I-PHONE_NUM  Thinking       1\n",
      "2           34      2  I-PHONE_NUM         –       2\n",
      "3           34      3  I-PHONE_NUM     Edgar       3\n",
      "4           34      4  I-PHONE_NUM      Lara       4\n",
      "...        ...    ...          ...       ...     ...\n",
      "1256        64    848  I-PHONE_NUM       was    1256\n",
      "1257        64    849  I-PHONE_NUM    needed    1257\n",
      "1258        64    850  I-PHONE_NUM        or    1258\n",
      "1259        64    851  I-PHONE_NUM  executed    1259\n",
      "1260        64    852  I-PHONE_NUM         .    1260\n",
      "\n",
      "[1261 rows x 5 columns]\n",
      "epoch 0 validation loss 2.624746322631836\n",
      "epoch 0 validation scores {'ents_p': 0.0, 'ents_r': 0.0, 'ents_f5': 0.0, 'ents_per_type': {'URL_PERSONAL': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'ID_NUM': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'PHONE_NUM': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'NAME_STUDENT': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'EMAIL': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'USERNAME': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'STREET_ADDRESS': {'p': 0.0, 'r': 0.0, 'f5': 0.0}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79785aa18223470cbbe91261adf09e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = DebertaLSTMModel(dm=dm)\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self):\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(Config.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(Config.model_path)        \n",
    "        # # Load the fine-tuned adapter layer on top of base model\n",
    "        # self.model = self.model.to(DEVICE)n\n",
    "        print(\"Complete loading pretrained LLM model\")     \n",
    "    \n",
    "    def infer_preds(self, ds: Dataset):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "# config = json.load(open(Path(Config.model_path) / \"config.json\"))\n",
    "# id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer()\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for preds, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(preds, offsets):\n",
    "        try:\n",
    "            label_pred = Config.id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "             # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "    \n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
