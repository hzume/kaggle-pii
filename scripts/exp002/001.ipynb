{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = \"002\"\n",
    "    ver = \"001\"\n",
    "    training = True\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    num_proc = 4\n",
    "\n",
    "    threshold = 0.99\n",
    "\n",
    "    tokenize_options = {\n",
    "        \"return_offsets_mapping\": True,\n",
    "        \"truncation\": False,\n",
    "        \"max_length\": 4096,\n",
    "    }\n",
    "    deberta_options = {\n",
    "        \"output_hidden_states\": True,\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"layer_norm_eps\": 1e-7,\n",
    "        \"add_pooling_layer\": False,\n",
    "    }\n",
    "\n",
    "    model_name = \"deberta3base-truncation-false\"\n",
    "    freeze_layers = 0\n",
    "\n",
    "    save_path = f\"/kaggle/input/{model_name}-{exp}-{ver}\"\n",
    "\n",
    "    output_dir = \"/kaggle/output\"\n",
    "\n",
    "    model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-small\"\n",
    "\n",
    "    train_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n",
    "    test_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
    "    moredata_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"\n",
    "    pii_dataset_fixed_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"\n",
    "\n",
    "    batch_size = 1\n",
    "    epochs = 3\n",
    "    lr = 1e-5\n",
    "\n",
    "    all_labels = [\n",
    "        \"B-EMAIL\",\n",
    "        \"B-ID_NUM\",\n",
    "        \"B-NAME_STUDENT\",\n",
    "        \"B-PHONE_NUM\",\n",
    "        \"B-STREET_ADDRESS\",\n",
    "        \"B-URL_PERSONAL\",\n",
    "        \"B-USERNAME\",\n",
    "        \"I-ID_NUM\",\n",
    "        \"I-NAME_STUDENT\",\n",
    "        \"I-PHONE_NUM\",\n",
    "        \"I-STREET_ADDRESS\",\n",
    "        \"I-URL_PERSONAL\",\n",
    "        \"O\",\n",
    "    ]\n",
    "    num_pii_labels = len(all_labels) - 1\n",
    "    label2id = {label: index for index, label in enumerate(all_labels)}\n",
    "    id2label = {index: label for index, label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    DebertaV2Config,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "# from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(Path(Config.train_path))\n",
    "    print(f\"kaggle train data = {len(train_data)}\")\n",
    "\n",
    "    more_data = pd.read_json(Path(Config.moredata_path))\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "\n",
    "    pii_dataset_fixed = pd.read_json(Path(Config.pii_dataset_fixed_path))\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    df[\"document\"] = [i for i in range(len(df))]  # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=Config.seed)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_true_samples = len(true_labels) - int(300 * len(true_labels) / len(df))\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_true_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - (300 - int(300 * len(true_labels) / len(df)))\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, is_test: bool) -> None:\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def tokenize_train(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        labels = []\n",
    "        targets = []\n",
    "        idx = 0\n",
    "        for t, l, ws in zip(row[\"tokens\"], row[\"labels\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            labels.extend([l]*len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if l in Config.all_labels:  \n",
    "                targets.append(1)\n",
    "            else:\n",
    "                targets.append(0)\n",
    "            \n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "            idx += 1\n",
    "\n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "         \n",
    "        target_num = sum(targets)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            if start_idx == 0 and end_idx == 0: \n",
    "                token_labels.append(Config.label2id[\"O\"])\n",
    "                continue\n",
    "            \n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            try:\n",
    "                token_labels.append(Config.label2id[labels[start_idx]])\n",
    "            except:\n",
    "                continue\n",
    "        length = len(tokenized.input_ids)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"labels\": token_labels,\n",
    "            \"length\": length,\n",
    "            \"target_num\": target_num,\n",
    "            \"group\": 1 if target_num > 0 else 0,\n",
    "            \"token_map\": token_map,\n",
    "            # \"tokens\": row[\"tokens\"],\n",
    "        }\n",
    "\n",
    "    def tokenize_test(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        \n",
    "        idx = 0\n",
    "        for t, ws in zip(row[\"tokens\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            token_map.extend([idx]*len(t))\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                token_map.append(-1)\n",
    "                \n",
    "            idx += 1\n",
    "            \n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"token_map\": token_map,\n",
    "            # \"tokens\": row[\"tokens\"],\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        if self.is_test:\n",
    "            return self.tokenize_test(row)\n",
    "        else:\n",
    "            return self.tokenize_train(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    DataFrameからモデリング時に使用するDataModuleを作成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        valid_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = Config.batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collator = DataCollatorForTokenClassification(\n",
    "            tokenizer, pad_to_multiple_of=512\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CreateDataset(\n",
    "            self.train_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.valid_dataset = CreateDataset(\n",
    "            self.valid_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.test_dataset = CreateDataset(\n",
    "            self.test_df, self.tokenizer, is_test=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dfs():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    # Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "    train_df, valid_df = downsample_df(df.copy())\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    valid_df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Number of train_df = {len(train_df)}\")\n",
    "    print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "\n",
    "    test_df = pd.read_json(Path(Config.test_path))\n",
    "    clear_memory()\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "more data = 2000\n",
      "pii_dataset_fixed = 4434\n",
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7203 true_others = 166\n",
      "false_samples = 5738 false_others = 134\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df, test_df = gen_dfs()\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_path)\n",
    "dm = CreateDataModule(train_df, valid_df, test_df, tokenizer)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_preds(logits_list: list[torch.Tensor], is_train: bool = True):\n",
    "    preds_final = []\n",
    "    for logits in logits_list:\n",
    "        logits = logits.cpu().detach().numpy()\n",
    "        if is_train:\n",
    "            logits_softmax = np.exp(logits) / np.sum(\n",
    "                np.exp(logits), axis=1\n",
    "            ).repeat(logits.shape[1]).reshape(logits.shape[0], logits.shape[1])\n",
    "        else:\n",
    "            logits_softmax = np.exp(logits) / np.sum(\n",
    "                np.exp(logits), axis=1\n",
    "            ).repeat(logits.shape[1]).reshape(logits.shape[0], logits.shape[1])\n",
    "        # Get the maximal value as the final preds\n",
    "        preds = logits_softmax.argmax(-1)\n",
    "        preds_without_O = logits_softmax[:, : Config.num_pii_labels].argmax(\n",
    "            -1\n",
    "        )  # Prob of entity labels (like 'NAME_STUDENT')\n",
    "        O_preds = logits_softmax[:, Config.num_pii_labels]  # Prob for 'O'\n",
    "\n",
    "        preds_final.append(np.where(O_preds < Config.threshold, preds_without_O, preds))\n",
    "    return preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(label_pred: list[list[str]], label_gt: list[list[str]]):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        \n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(label_pred, label_gt)\n",
    "        precision = precision_score(label_pred, label_gt)\n",
    "        # f5 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_df(\n",
    "    preds_list: list[torch.Tensor], valid_dataset: CreateDataset, valid_df: pd.DataFrame\n",
    "):\n",
    "    pairs = set()\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for index, (preds, row) in enumerate(zip(preds_list, valid_dataset)):\n",
    "        print(preds)\n",
    "        for p, token_map, offsets, tokens in zip(\n",
    "            preds,\n",
    "            row[\"token_map\"],\n",
    "            row[\"offset_mapping\"],\n",
    "            valid_df.iloc[index][\"tokens\"],            \n",
    "        ):\n",
    "            doc = valid_df.iloc[index][\"document\"]\n",
    "            # p = p.argmax(-1).cpu().detach().numpy()\n",
    "            # p = p.cpu().detach().numpy()\n",
    "\n",
    "            for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "                label_pred = Config.id2label[(token_pred)]\n",
    "\n",
    "                if start_idx + end_idx == 0:\n",
    "                    continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while (\n",
    "                    start_idx < len(token_map)\n",
    "                    and tokens[token_map[start_idx]].isspace()\n",
    "                ):\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map):\n",
    "                    break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "\n",
    "                if label_pred == \"O\" or token_id == -1:\n",
    "                    continue\n",
    "\n",
    "                pair = (doc, token_id)\n",
    "\n",
    "                if pair in pairs:\n",
    "                    continue\n",
    "\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                pairs.add(pair)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": document, \"token\": token, \"label\": label, \"token_str\": token_str}\n",
    "    )\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (_, _) = self.lstm.forward(x)\n",
    "        out = hidden\n",
    "        return out\n",
    "\n",
    "\n",
    "class DebertaLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, dm: CreateDataModule):\n",
    "        super(DebertaLSTMModel, self).__init__()\n",
    "\n",
    "        self.dm = dm\n",
    "\n",
    "        self.model_config: DebertaV2Config = AutoConfig.from_pretrained(\n",
    "            Config.model_path\n",
    "        )\n",
    "\n",
    "        self.model_config.update(Config.deberta_options)\n",
    "\n",
    "        self.transformers_model: PreTrainedModel = AutoModel.from_pretrained(\n",
    "            Config.model_path\n",
    "        )\n",
    "        self.head = LSTMHead(\n",
    "            in_features=self.model_config.hidden_size,\n",
    "            hidden_dim=self.model_config.hidden_size // 2,\n",
    "            n_layers=1,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.model_config.hidden_size, len(Config.all_labels))\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='mean',ignore_index=-100) \n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # if Config.freeze_layers>0:\n",
    "        # \tprint(f'Freezing {Config.freeze_layers} layers.')\n",
    "        # \tfor layer in self.transformers_model.longformer.encoder.layer[:Config.freeze_layers]:\n",
    "        # \t\tfor param in layer.parameters():\n",
    "        # \t\t\tparam.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, train=True):\n",
    "        transformer_out: BaseModelOutput = self.transformers_model.forward(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = transformer_out.last_hidden_state\n",
    "        head_output = self.head.forward(last_hidden_state)\n",
    "        # last_hidden_state = (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        logits = self.fc.forward(head_output)\n",
    "\n",
    "        # logits = (batch_size, seq_len, num_labels)\n",
    "        return (logits, _)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        outputs = self.forward(input_ids, attention_mask, train=True)\n",
    "\n",
    "        ## only batch_size=1\n",
    "        output = outputs[0]\n",
    "        # output = (seq_len, num_labels)\n",
    "        loss = self.loss_function(\n",
    "            output.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def train_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        print(f\"epoch {self.trainer.current_epoch} training loss {avg_loss}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        outputs = self.forward(input_ids, attention_mask, train=False)\n",
    "        output = outputs[0]\n",
    "\n",
    "        loss = self.loss_function(\n",
    "            output.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.validation_step_outputs.append(\n",
    "            {\"val_loss\": loss, \"logits\": output, \"targets\": target, \"row\": batch}\n",
    "        )\n",
    "        return {\"val_loss\": loss, \"logits\": output, \"targets\": target}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        logits_list = [logits for batch in outputs for logits in batch[\"logits\"]]\n",
    "        print(type(logits_list[0]))\n",
    "        preds_list = post_processing_preds(logits_list)\n",
    "        # print(flattened_preds.shape)\n",
    "        pred_df = predictions_to_df(preds_list, self.dm.valid_dataset, self.dm.valid_df)\n",
    "\n",
    "        print(pred_df.shape)\n",
    "        print(pred_df)\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # print(output_val.shape)\n",
    "        labels_gt = [target for batch in outputs for target in batch[\"targets\"]]\n",
    "        avg_score = compute_metrics([pred_df[\"label\"].to_list()], [labels_gt])\n",
    "        f5_score = avg_score[\"ents_f5\"]\n",
    "        print(f\"epoch {self.trainer.current_epoch} validation loss {avg_loss}\")\n",
    "        print(f\"epoch {self.trainer.current_epoch} validation scores {avg_score}\")\n",
    "\n",
    "        return {\"val_loss\": avg_loss, \"val_f5\": f5_score}\n",
    "\n",
    "    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"transformers_model\" not in n\n",
    "                ],\n",
    "                \"lr\": decoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=Config.lr)\n",
    "\n",
    "        epoch_steps = len(self.dm.train_dataset)\n",
    "        batch_size = Config.batch_size\n",
    "\n",
    "        warmup_steps = 0.05 * epoch_steps // batch_size\n",
    "        training_steps = Config.epochs * epoch_steps // batch_size\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n",
    "        # scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=1e-6, power=3.0)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, warmup_steps, training_steps, num_cycles=0.5\n",
    "        )\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:72: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f87acec6cf4842a3f1376c3aa43d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "[10 10 10 10 10 10 10 10  0 10 10 10 10  4 10 10  3  3  2 10  2  5  5  4\n",
      "  3  5 11 11  3  4  4 11  1  3  3  3  3  3  3  1  3 10 10  3  4  3 10 10\n",
      " 11 11  4  5  1  4  4  5 10  4  4  4  4  4 11  3  1 10  4 11  3  3  1  3\n",
      "  4  4  4 10  8  3 11  1  1  1  4  1  4  4  0  4  4  4  4  8  6  8  3  5\n",
      "  5  3 11 11 10  4  4  4  8  4  1 10 10  4  4  4  4  8 10 10 10  3  3  1\n",
      "  1 11  4 11 11  8  5  1  4  1  1  1  5  8  8  4  4 10  1  4 11 10 10 10\n",
      " 11  5  1  3 10 10  1 10 10 10  4  4  1 10  3  4  4  9 10  4  4 11  1 10\n",
      "  2  3  5  3  3  3 10  3  3  3 11  3 10 10  9  3  1 10 10 10 10 10 10 10\n",
      "  3  1  1  5  3  4  1 11  1  4  4  1  5  5  5  3  4  3  3  4 10 10 10  4\n",
      "  4  4  4  4  4  1 11 10 10  1  8 10 10 10 10 10 10 10  1 10 10  1 10 11\n",
      " 11 11  4 11  1  3  1  9  9  4  4  3  3  5  4  4  3 10  1  1 10  1 10  1\n",
      " 10  1  3  9  4  3  4  9  4  8  4 10 11 11 10 10  4  3  3  5  8  8  4 11\n",
      "  5  4 10  5 10 10  8 10  3 10 10 10  5  2 10  3  5  4  3  3 10  3  3  3\n",
      "  4  5  8  8  8 10 10 10 11 11  9  3 11 11  9  3 10  3 10  1 11  1  1  5\n",
      "  5 10 10 10 10  8  0  0  9  2  3 11  3  3 11  3  3  3  3 10  3 10  4  8\n",
      " 10  9  3  3  9 10  9 10  8 10 10  8 11 11 11  4  3  4  4  4  4 10  3  4\n",
      "  4  9  4 10 11  8  2  9 11 10 10 10 11 11 10  1 10 10 10 10  5  8  9  9\n",
      "  9  1  1 11 11 11  4 11  3 10  4 10  9 10 10 10  1 10  4 10 10 11 10 10\n",
      "  4  3  3  3 10  5  8  3  3  3  3  1  1  1  1 11 11  4  3  3  1  1  3  3\n",
      " 10  1  3 11  1  1  5  5  3  3  3  3  3  9 11  3 11  1  1 10 10 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DebertaLSTMModel(dm\u001b[38;5;241m=\u001b[39mdm)\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:244\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:326\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[0;32m--> 326\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:146\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    149\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[68], line 112\u001b[0m, in \u001b[0;36mDebertaLSTMModel.on_validation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m preds_list \u001b[38;5;241m=\u001b[39m post_processing_preds(logits_list)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(flattened_preds.shape)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_df)\n",
      "Cell \u001b[0;32mIn[67], line 16\u001b[0m, in \u001b[0;36mpredictions_to_df\u001b[0;34m(preds_list, valid_dataset, valid_df)\u001b[0m\n\u001b[1;32m     14\u001b[0m doc \u001b[38;5;241m=\u001b[39m valid_df\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# p = p.argmax(-1).cpu().detach().numpy()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_pred, (start_idx, end_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(p, offsets):\n\u001b[1;32m     19\u001b[0m     label_pred \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mid2label[(token_pred)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "model = DebertaLSTMModel(dm=dm)\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self):\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(Config.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(Config.model_path)        \n",
    "        # # Load the fine-tuned adapter layer on top of base model\n",
    "        # self.model = self.model.to(DEVICE)n\n",
    "        print(\"Complete loading pretrained LLM model\")     \n",
    "    \n",
    "    def infer_preds(self, ds: Dataset):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "# config = json.load(open(Path(Config.model_path) / \"config.json\"))\n",
    "# id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer()\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        try:\n",
    "            label_pred = Config.id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "             # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "    \n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
