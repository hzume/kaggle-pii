{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (597210843.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    freeze_layers = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "\texp = \"002\"\n",
    "\tver = \"001\"\n",
    "\t\n",
    "\tseed = 42\n",
    "\n",
    "\tnum_proc = 4\n",
    "\n",
    "\tthreshold = 0.99\n",
    "\tmax_length = 1024\n",
    "\n",
    "\tmodel_name = \"deberta3base-truncation-false\"\n",
    "\tfreeze_layers = 0\n",
    "\n",
    "\tsave_path = f\"/kaggle/input/{model_name}-{exp}-{ver}\"\n",
    "\n",
    "\toutput_dir = \"/kaggle/output\"\n",
    "\n",
    "\tmodel_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-small\"\n",
    "\t\n",
    "\ttrain_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n",
    "\ttest_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
    "\tmoredata_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"\n",
    "\tpii_dataset_fixed_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"\n",
    "\n",
    "\tall_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n",
    "\tnum_pii_labels = len(all_labels)-1\n",
    "\tlabel2id = {label: index for index, label in enumerate(all_labels)}\n",
    "\tid2label = {index: label for index, label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "\tAutoTokenizer,\n",
    "\tTrainer,\n",
    "\tTrainingArguments,\n",
    "\tAutoConfig,\n",
    "\tAutoModel,\n",
    "\tAutoModelForTokenClassification,\n",
    "\tDataCollatorForTokenClassification,\n",
    "\tPreTrainedTokenizer,\n",
    "\tPreTrainedModel,\n",
    "\tPretrainedConfig,\n",
    "\tDebertaV2Config,\n",
    ")\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "\tlibc.malloc_trim(0)\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tgc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\t# Load training data\n",
    "\ttrain_data = pd.read_json(Path(Config.train_path))\n",
    "\tprint(f\"kaggle train data = {len(train_data)}\")\n",
    "\n",
    "\tmore_data = pd.read_json(Path(Config.moredata_path))\n",
    "\tprint(f\"more data = {len(more_data)}\")\n",
    "\n",
    "\tpii_dataset_fixed = pd.read_json(Path(Config.pii_dataset_fixed_path))\n",
    "\tprint(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "\t# Combine to a single df\n",
    "\tdf = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "\tdf[\"document\"] = [i for i in range(len(df))]  # Update the document id\n",
    "\tdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "# Eencode labels to columns\n",
    "def encode_labels(df: pd.DataFrame):\n",
    "\ttotal = len(df)\n",
    "\tdf[\"unique_labels\"] = df[\"labels\"].apply(\n",
    "\t\tlambda labels: list(\n",
    "\t\t\tset([label.split(\"-\")[1] for label in labels if label != \"O\"])\n",
    "\t\t)\n",
    "\t)\n",
    "\tmlb = MultiLabelBinarizer()\n",
    "\tone_hot_encoded = mlb.fit_transform(df[\"unique_labels\"])\n",
    "\tone_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "\tdf = pd.concat([df, one_hot_df], axis=1)\n",
    "\t# add 'POS' column that don't have\n",
    "\tdf[\"others\"] = df[\"unique_labels\"].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "\tlabel_classes = list(mlb.classes_) + [\"others\"]\n",
    "\tfor col in label_classes:\n",
    "\t\tsubtotal = df[col].sum()\n",
    "\t\tpercent = subtotal / total * 100\n",
    "\t\tprint(f\"{col}: {subtotal}  ({percent:.1f}%)\")\n",
    "\treturn df, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "\t# Get the sample df\n",
    "\tsamples_df = df.sample(n=n_samples, random_state=Config.seed)\n",
    "\t# The remaining df\n",
    "\tcond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "\tothers_df = df.drop(df[cond].index, inplace=False)\n",
    "\treturn samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "\t\"\"\"Split the df into training and valid dataset\"\"\"\n",
    "\tdf[\"is_labels\"] = df[\"labels\"].apply(\n",
    "\t\tlambda labels: any(label != \"O\" for label in labels)\n",
    "\t)\n",
    "\n",
    "\t# One or more labels are not 'O'\n",
    "\ttrue_labels = df[df[\"is_labels\"]]\n",
    "\t# all labels are 'O'\n",
    "\tfalse_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "\t# Reset index to two df\n",
    "\ttrue_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "\tfalse_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "\tprint(f\"Number of true_labels = {len(true_labels)}\")\n",
    "\tprint(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "\t# Get 300 as valid dataset\n",
    "\tn_true_samples = len(true_labels) - int(300 * len(true_labels) / len(df))\n",
    "\n",
    "\t# Sample true labels\n",
    "\ttrue_samples, true_others = split_df_by_sampling(true_labels, n_true_samples)\n",
    "\tprint(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "\tn_samples = len(false_labels) - (300 - int(300 * len(true_labels) / len(df)))\n",
    "\t# Sample false labels\n",
    "\tfalse_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "\tprint(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "\t# Training ds = P * true_labels + P * false_labels\n",
    "\ttrain_df = pd.concat([true_samples, false_samples])\n",
    "\t# Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "\tvalid_df = pd.concat([true_others, false_others])\n",
    "\treturn train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(example: pd.DataFrame, tokenizer: PreTrainedTokenizer, label2id: dict[str, int]):\n",
    "\t# Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "\ttokens = []\n",
    "\tlabels = []\n",
    "\tfor token, label, t_ws in zip(\n",
    "\t\texample[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "\t):\n",
    "\t\ttokens.append(token)\n",
    "\t\tlabels.extend([label] * len(token))\n",
    "\t\t# Added trailing whitespace and label if true and\n",
    "\t\tif t_ws:\n",
    "\t\t\ttokens.append(\" \")\n",
    "\t\t\tlabels.append(\"O\")\n",
    "\n",
    "\ttext = \"\".join(tokens)\n",
    "\t# print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "\t# tokenization without truncation\n",
    "\ttokenized = tokenizer(text, return_offsets_mapping=True, truncation=False)\n",
    "\tlabels = np.array(labels)\n",
    "\t# Labels\n",
    "\ttoken_labels = []\n",
    "\tfor start_idx, end_idx in tokenized.offset_mapping:\n",
    "\t\t# Added 'O'\n",
    "\t\tif start_idx == 0 and end_idx == 0:\n",
    "\t\t\ttoken_labels.append(label2id[\"O\"])\n",
    "\t\telse:\n",
    "\t\t\t# case when the text starts with whitespace\n",
    "\t\t\tif text[start_idx].isspace():\n",
    "\t\t\t\tstart_idx += 1\n",
    "\t\t\t# Convert label to id (int)\n",
    "\t\t\tlabel_id = label2id[labels[start_idx]]\n",
    "\t\t\ttoken_labels.append(label_id)\n",
    "\n",
    "\treturn {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}\n",
    "\n",
    "# Convert df to tokenized dataset\n",
    "def create_dataset(df: pd.DataFrame, tokenizer: PreTrainedTokenizer, label2id: dict[str, int]):\n",
    "\tds = Dataset.from_dict(\n",
    "\t\t{\n",
    "\t\t\t\"full_text\": df[\"full_text\"].tolist(),\n",
    "\t\t\t\"document\": df[\"document\"].astype(\"string\"),\n",
    "\t\t\t\"tokens\": df[\"tokens\"].tolist(),\n",
    "\t\t\t\"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "\t\t\t\"provided_labels\": df[\"labels\"].tolist(),\n",
    "\t\t}\n",
    "\t)\n",
    "\t# Tokenize the dataset\n",
    "\ttokenized_ds = ds.map(\n",
    "\t\ttokenize,\n",
    "\t\tfn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
    "\t\tnum_proc=Config.num_proc,\n",
    "\t)\n",
    "\treturn tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "# Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "train_df, valid_df = downsample_df(df.copy())\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_path)\n",
    "train_ds = create_dataset(train_df, tokenizer, Config.label2id)\n",
    "valid_ds = create_dataset(valid_df, tokenizer, Config.label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_preds(preds: torch.Tensor, is_train: bool = True):\n",
    "\tpreds_final = []\n",
    "\tif is_train:\n",
    "\t\tpreds_softmax: torch.Tensor = np.exp(preds) / np.sum(\n",
    "\t\t\tnp.exp(preds), axis=2\n",
    "\t\t).reshape(preds.shape[0], preds.shape[1], 1)\n",
    "\telse:\n",
    "\t\tpreds_softmax: torch.Tensor = np.exp(preds) / np.sum(\n",
    "\t\t\tnp.exp(preds), axis=2\n",
    "\t\t).reshape(preds.shape[0], preds.shape[1], -1)\n",
    "\t# Get the maximal value as the final preds\n",
    "\tpreds = preds.argmax(-1)\n",
    "\tpreds_without_O = preds_softmax[:, :, : Config.num_pii_labels].argmax(\n",
    "\t\t-1\n",
    "\t)  # Prob of entity labels (like 'NAME_STUDENT')\n",
    "\tO_preds = preds_softmax[:, :, Config.num_pii_labels]  # Prob for 'O'\n",
    "\n",
    "\tpreds_final = np.where(O_preds < Config.threshold, preds_without_O, preds)\n",
    "\treturn preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(preds, all_labels):    \n",
    "\ttry:\n",
    "\t\t#print(\"Compute metrics\")\n",
    "\t\tpredictions, labels = preds\n",
    "\t\t# predictions = np.argmax(predictions, axis=2)\n",
    "\t\tpredictions = post_processing_preds(predictions)\n",
    "\t\t# Include prediction Remove ignored index (special tokens)\n",
    "\t\ttrue_preds = []\n",
    "\t\ttrue_labels = []\n",
    "\t\tfor pred, label in zip(predictions, labels):\n",
    "\t\t\ttrue_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "\t\t\ttrue_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "\t\n",
    "\t\t# Compute recall, precision and f1 score\n",
    "\t\trecall = recall_score(true_labels, true_preds)\n",
    "\t\tprecision = precision_score(true_labels, true_preds)\n",
    "\t\t# f5 score to measure the performance\n",
    "\t\tf5_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "\t\tresult = {'f5': f5_score,  \n",
    "\t\t\t\t  'recall': recall,\n",
    "\t\t\t\t  'precision': precision}\n",
    "\t\tprint(f\"result = {result}\")\n",
    "\t\treturn result\n",
    "\texcept Exception as e: \n",
    "\t\tprint(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (2993439985.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.head = LSTMHead(in_features=self.model_config.hidden_size, hidden_dim=self.model_config.hidden_size//2, n_layers=1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class LSTMHead(nn.Module):\n",
    "\tdef __init__(self, in_features, hidden_dim, n_layers):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.lstm = nn.LSTM(\n",
    "\t\t\tin_features,\n",
    "\t\t\thidden_dim,\n",
    "\t\t\tn_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tbidirectional=True,\n",
    "\t\t\tdropout=0.1,\n",
    "\t\t)\n",
    "\t\tself.out_features = hidden_dim\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.lstm.flatten_parameters()\n",
    "\t\thidden, (_, _) = self.lstm(x)\n",
    "\t\tout = hidden\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class DebertaLSTMModel(pl.LightningModule):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DebertaLSTMModel, self).__init__()\n",
    "\t\tself.model_config: DebertaV2Config = AutoConfig.from_pretrained(Config.model_path)\n",
    "\t\t\n",
    "\t\thidden_dropout_prob: float = 0.1\n",
    "\t\tlayer_norm_eps: float = 1e-7\n",
    "\t\tself.model_config.update(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"output_hidden_states\": True,\n",
    "\t\t\t\t\"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "\t\t\t\t\"layer_norm_eps\": layer_norm_eps,\n",
    "\t\t\t\t\"add_pooling_layer\": False,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\tself.transformers_model: PreTrainedModel = AutoModel.from_pretrained(\n",
    "\t\t\tConfig.model_path\n",
    "\t\t)\n",
    "\t\tself.head = LSTMHead(in_features=self.model_config.hidden_size, hidden_dim=self.model_config.hidden_size//2, n_layers=1)\n",
    "\t\t\n",
    "\t\tself.output = nn.Linear(self.model_config.hidden_size, len(self.cfg.target_cols))\n",
    "\t\t\n",
    "\t\tif Config.freeze_layers>0:\n",
    "\t\t\tprint(f'Freezing {Config.freeze_layers} layers.')\n",
    "\t\t\tfor layer in self.transformers_model.longformer.encoder.layer[:config.freeze_layers]:\n",
    "\t\t\t\tfor param in layer.parameters():\n",
    "\t\t\t\t\tparam.requires_grad = False\n",
    "\n",
    "\n",
    "\t\tself.loss_function = nn.CrossEntropyLoss(reduction='mean',ignore_index=-100) \n",
    "\t\tself.validation_step_outputs = []\n",
    "\n",
    "\tdef forward(self, input_ids, attention_mask,train):        \n",
    "\t\ttransformer_out = self.transformers_model(input_ids,attention_mask = attention_mask)\n",
    "\t\tsequence_output = transformer_out.last_hidden_state\n",
    "\t\tsequence_output = self.head(sequence_output)\n",
    "\t\tlogits = self.output(sequence_output)\n",
    "\n",
    "\t\treturn (logits, _)\n",
    "\t\n",
    "\n",
    "\tdef training_step(self,batch,batch_idx):\n",
    "\t\tinput_ids = batch['input_ids']\n",
    "\t\tattention_mask = batch['attention_mask']\n",
    "\t\ttarget = batch['labels'] \n",
    "\n",
    "\t\toutputs = self(input_ids,attention_mask,train=True)\n",
    "\t\toutput = outputs[0]\n",
    "\t\tloss = self.loss_function(output.view(-1,len(self.cfg.target_cols)), target.view(-1))\n",
    "\t\t\n",
    "\t\tself.log('train_loss', loss , prog_bar=True)\n",
    "\t\treturn {'loss': loss}\n",
    "\n",
    "\tdef train_epoch_end(self,outputs):\n",
    "\t\tavg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\t\tprint(f'epoch {trainer.current_epoch} training loss {avg_loss}')\n",
    "\t\treturn {'train_loss': avg_loss} \n",
    "\t\n",
    "\tdef validation_step(self,batch,batch_idx):\n",
    "\t\tinput_ids = batch['input_ids']\n",
    "\t\tattention_mask = batch['attention_mask']\n",
    "\t\ttarget = batch['labels'] \n",
    "\n",
    "\t\toutputs = self(input_ids,attention_mask,train=False)\n",
    "\t\toutput = outputs[0]\n",
    "\n",
    "\t\tloss = self.loss_function(output.view(-1,len(self.cfg.target_cols)), target.view(-1))\n",
    "\t\t\n",
    "\t\tself.log('val_loss', loss , prog_bar=True)\n",
    "\t\tself.validation_step_outputs.append({\"val_loss\": loss, \"logits\": output, \"targets\": target})\n",
    "\t\treturn {'val_loss': loss, 'logits': output,'targets':target}\n",
    "\n",
    "\tdef on_validation_epoch_end(self):\n",
    "\t\toutputs = self.validation_step_outputs\n",
    "\t\tavg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "\n",
    "\t\tflattened_preds = [logit for batch in outputs for logit in batch['logits']]\n",
    "\n",
    "\t\tflattened_preds = process_predictions(flattened_preds)\n",
    "\t\t# print(flattened_preds.shape)\n",
    "\t\tpred_df = predictions_to_df(flattened_preds, self.val_ds)\n",
    "\t\t\n",
    "\t\tprint(pred_df.shape)\n",
    "\t\tprint(pred_df)\n",
    "\t\t\n",
    "\t\tself.validation_step_outputs = []\n",
    "\n",
    "\t\t# print(output_val.shape)\n",
    "\t\tavg_score = compute_metrics(pred_df,self.true_val_df)\n",
    "\t\tf5_score = avg_score['ents_f5']\n",
    "\t\tprint(f'epoch {trainer.current_epoch} validation loss {avg_loss}')\n",
    "\t\tprint(f'epoch {trainer.current_epoch} validation scores {avg_score}')\n",
    "\t\t\n",
    "\t\treturn {'val_loss': avg_loss,'val_f5':f5_score}\n",
    "\t\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn self._train_dataloader \n",
    "\t\n",
    "\tdef validation_dataloader(self):\n",
    "\t\treturn self._validation_dataloader\n",
    "\n",
    "\tdef get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "\t\tparam_optimizer = list(model.named_parameters())\n",
    "\t\tno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\t\toptimizer_parameters = [\n",
    "\t\t\t{'params': [p for n, p in self.transformers_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "\t\t\t 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "\t\t\t{'params': [p for n, p in self.transformers_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "\t\t\t 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "\t\t\t{'params': [p for n, p in self.named_parameters() if \"transformers_model\" not in n],\n",
    "\t\t\t 'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "\t\t]\n",
    "\t\treturn optimizer_parameters\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\toptimizer = AdamW(self.parameters(), lr = config.learning_rate)\n",
    "\n",
    "\t\tepoch_steps = self.cfg.data_length\n",
    "\t\tbatch_size = self.cfg.batch_size\n",
    "\n",
    "\t\twarmup_steps = 0.05 * epoch_steps // batch_size\n",
    "\t\ttraining_steps = self.cfg.epochs * epoch_steps // batch_size\n",
    "\t\t# scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n",
    "\t\t# scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=1e-6, power=3.0)\n",
    "\t\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, training_steps, num_cycles=0.5)\n",
    "\t\t\n",
    "\t\tlr_scheduler_config = {\n",
    "\t\t\t\t'scheduler': scheduler,\n",
    "\t\t\t\t'interval': 'step',\n",
    "\t\t\t\t'frequency': 1,\n",
    "\t\t\t}\n",
    "\n",
    "\t\treturn {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training requires the GPUs and internet\n",
    "# TRAINING = True # True: Model Training, False: Model Inference\n",
    "# if TRAINING: \n",
    "#     # Configuration class containing various model and training parameters\n",
    "#     trainer = ModelTrainer()\n",
    "#     trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "\tdef __init__(self):\n",
    "\t\tself.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "\t\tself.load_model()\n",
    "\n",
    "\tdef load_model(self):\n",
    "\t\t# Create the tokenizer\n",
    "\t\tself.tokenizer = AutoTokenizer.from_pretrained(Config.model_path) \n",
    "\t\t# Create the model\n",
    "\t\tself.model = AutoModelForTokenClassification.from_pretrained(Config.model_path)        \n",
    "\t\t# # Load the fine-tuned adapter layer on top of base model\n",
    "\t\t# self.model = self.model.to(DEVICE)n\n",
    "\t\tprint(\"Complete loading pretrained LLM model\")     \n",
    "\t\n",
    "\tdef infer_preds(self, ds: Dataset):\n",
    "\t\t# Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "\t\ttokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "\t\t# Create data loader\n",
    "\t\tdata_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   pad_to_multiple_of=16)\n",
    "\t\t# Arguments (infer only)\n",
    "\t\targs = TrainingArguments(output_dir=self.infer_dir,\n",
    "\t\t\t\t\t\t\t\t per_device_eval_batch_size=1, \n",
    "\t\t\t\t\t\t\t\t report_to=\"none\")\n",
    "\t\t# Create the trainer \n",
    "\t\ttrainer = Trainer(model=self.model, \n",
    "\t\t\t\t\t\t  args=args, \n",
    "\t\t\t\t\t\t  data_collator=data_collator, \n",
    "\t\t\t\t\t\t  tokenizer=self.tokenizer)\n",
    "\t\t\n",
    "\t\t# predict for that split\n",
    "\t\tpreds = trainer.predict(tokenized_ds).predictions\n",
    "\t\t\t\t\n",
    "\t\t# Clear the unused memory\n",
    "\t\tdel self.model, data_collator, trainer, args \n",
    "\t\tclear_memory()\n",
    "\t\tpreds_final = post_processing_preds(preds)\n",
    "\t\treturn preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "\t\"full_text\": test_data[\"full_text\"].tolist(),\n",
    "\t\"document\": test_data[\"document\"].tolist(),\n",
    "\t\"tokens\": test_data[\"tokens\"].tolist(),\n",
    "\t\"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "# config = json.load(open(Path(Config.model_path) / \"config.json\"))\n",
    "# id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer()\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  tokenized_ds[\"token_map\"], \n",
    "\t\t\t\t\t\t\t\t\t\t\t  tokenized_ds[\"offset_mapping\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t  tokenized_ds[\"tokens\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t  tokenized_ds[\"document\"]):\n",
    "\tfor token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "\t\ttry:\n",
    "\t\t\tlabel_pred = Config.id2label[str(token_pred)]\n",
    "\t\t\tif start_idx + end_idx == 0: \n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif token_map[start_idx] == -1:\n",
    "\t\t\t\tstart_idx += 1\n",
    "\t\t\t # ignore \"\\n\\n\"\n",
    "\t\t\twhile start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "\t\t\t\tstart_idx += 1\n",
    "\n",
    "\t\t\tif start_idx >= len(token_map): \n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\ttoken_id = token_map[start_idx]\n",
    "\n",
    "\t\t\t# ignore \"O\" predictions and whitespace preds\n",
    "\t\t\tif label_pred != \"O\" and token_id != -1:\n",
    "\t\t\t\tresults.append({\n",
    "\t\t\t\t\t\t\"document\": doc,\n",
    "\t\t\t\t\t\t\"token\": token_id,\n",
    "\t\t\t\t\t\t\"label\": label_pred,\n",
    "\t\t\t\t\t\t\"token_str\": tokens[token_id]\n",
    "\t\t\t\t\t})\n",
    "\t\t\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error {e}\")\n",
    "\t\t\tprint(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "\t\t\tsys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "\tidx = 0\n",
    "\tspans = []\n",
    "\tspan = []\n",
    "\t\n",
    "\tfor i, token in enumerate(document):\n",
    "\t\tif token != target[idx]:\n",
    "\t\t\tidx = 0\n",
    "\t\t\tspan = []\n",
    "\t\t\tcontinue\n",
    "\t\tspan.append(i)\n",
    "\t\t\n",
    "\t\tidx += 1\n",
    "\t\tif idx == len(target):\n",
    "\t\t\tspans.append(span)\n",
    "\t\t\tspan = []\n",
    "\t\t\tidx = 0\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\treturn spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "\t# email\n",
    "\tfor token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "\t\tif re.fullmatch(email_regex, token) is not None:\n",
    "\t\t\temails.append(\n",
    "\t\t\t\t{\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "\t\t\t)\n",
    "\t# phone number\n",
    "\tmatches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "\tif not matches:\n",
    "\t\tcontinue\n",
    "\t\t\n",
    "\tfor match in matches:\n",
    "\t\ttarget = [t.text for t in nlp.tokenizer(match)]\n",
    "\t\tmatched_spans = find_span(target, _data[\"tokens\"])\n",
    "\t\t\n",
    "\tfor matched_span in matched_spans:\n",
    "\t\tfor intermediate, token_idx in enumerate(matched_span):\n",
    "\t\t\tprefix = \"I\" if intermediate else \"B\"\n",
    "\t\t\tphone_nums.append(\n",
    "\t\t\t\t{\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "\t\t\t)\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "\t# Sort by the document and token\n",
    "\tdf.sort_values(by=['document', 'token'])\n",
    "\t# Combine three columns \n",
    "\tdf['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\t# display(df)\n",
    "\t# Drop duplicated triplets and keep the first one as unique row\n",
    "\tdf = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "\t# Regenerate 'row_id'\n",
    "\tdf['row_id'] = list(range(len(df)))    \n",
    "\tdf = df.reset_index(drop=True, inplace=False) \n",
    "\tprint(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
