{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = \"002\"\n",
    "    ver = \"002\"\n",
    "    training = True\n",
    "    resume = False\n",
    "\n",
    "    project_name = f'pii-{exp}-{ver}'\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    num_proc = 4\n",
    "\n",
    "    threshold = 0.99\n",
    "\n",
    "    tokenize_options = {\n",
    "        \"return_offsets_mapping\": True,\n",
    "        \"truncation\": False,\n",
    "        \"max_length\": 4096,\n",
    "    }\n",
    "    deberta_options = {\n",
    "        \"output_hidden_states\": True,\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"layer_norm_eps\": 1e-7,\n",
    "        \"add_pooling_layer\": False,\n",
    "    }\n",
    "\n",
    "    model_name = \"deberta3base-truncation-false\"\n",
    "    freeze_layers = 0\n",
    "\n",
    "    save_dir = f\"/kaggle/input/{project_name}\"\n",
    "    save_path = f\"{save_dir}/{model_name}.ckpt\"\n",
    "\n",
    "    output_dir = \"/kaggle/output\"\n",
    "\n",
    "    model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\"\n",
    "\n",
    "    train_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n",
    "    test_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
    "    moredata_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"\n",
    "    pii_dataset_fixed_path = \"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"\n",
    "    sample_sub_path = '/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv'\n",
    "\n",
    "    batch_size = 1\n",
    "    epochs = 3\n",
    "    lr = 1e-5\n",
    "\n",
    "    all_labels = [\n",
    "        \"B-EMAIL\",\n",
    "        \"B-ID_NUM\",\n",
    "        \"B-NAME_STUDENT\",\n",
    "        \"B-PHONE_NUM\",\n",
    "        \"B-STREET_ADDRESS\",\n",
    "        \"B-URL_PERSONAL\",\n",
    "        \"B-USERNAME\",\n",
    "        \"I-ID_NUM\",\n",
    "        \"I-NAME_STUDENT\",\n",
    "        \"I-PHONE_NUM\",\n",
    "        \"I-STREET_ADDRESS\",\n",
    "        \"I-URL_PERSONAL\",\n",
    "        \"O\",\n",
    "    ]\n",
    "    num_pii_labels = len(all_labels) - 1\n",
    "    label2id = {label: index for index, label in enumerate(all_labels)}\n",
    "    id2label = {index: label for index, label in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "sys.path.append('/kaggle/input/piimetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "from tqdm import tqdm\n",
    "# from rich import print\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from comp_metric import compute_metrics\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    DebertaV2Config,\n",
    "    DebertaV2Model,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(Path(Config.train_path))\n",
    "    print(f\"kaggle train data = {len(train_data)}\")\n",
    "\n",
    "    more_data = pd.read_json(Path(Config.moredata_path))\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "\n",
    "    pii_dataset_fixed = pd.read_json(Path(Config.pii_dataset_fixed_path))\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    df[\"document\"] = [i for i in range(len(df))]  # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=Config.seed)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_true_samples = len(true_labels) - int(300 * len(true_labels) / len(df))\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_true_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - (300 - int(300 * len(true_labels) / len(df)))\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, is_test: bool) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        if is_test:\n",
    "            self.tokenized_df = df.apply(self.tokenize_test, axis=1, result_type=\"expand\")\n",
    "        else:\n",
    "            self.tokenized_df = df.apply(self.tokenize_train, axis=1, result_type=\"expand\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_df)\n",
    "    \n",
    "    def tokenize_train(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        labels = []\n",
    "        targets = []\n",
    "        idx = 0\n",
    "        for t, l, ws in zip(row[\"tokens\"], row[\"labels\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            labels.extend([l]*len(t))\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if l in Config.all_labels:  \n",
    "                targets.append(1)\n",
    "            else:\n",
    "                targets.append(0)\n",
    "            \n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "                token_map.append(-1)\n",
    "            idx += 1\n",
    "\n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "         \n",
    "        target_num = sum(targets)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "            if start_idx == 0 and end_idx == 0: \n",
    "                token_labels.append(Config.label2id[\"O\"])\n",
    "                continue\n",
    "            \n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            try:\n",
    "                token_labels.append(Config.label2id[labels[start_idx]])\n",
    "            except:\n",
    "                continue\n",
    "        length = len(tokenized.input_ids)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"labels\": token_labels,\n",
    "            \"length\": length,\n",
    "            \"target_num\": target_num,\n",
    "            \"group\": 1 if target_num > 0 else 0,\n",
    "            \"token_map\": token_map,\n",
    "            \"document\": row[\"document\"],\n",
    "            \"tokens\": row[\"tokens\"],      \n",
    "            \"raw_labels\": row[\"labels\"],     \n",
    "        }\n",
    "\n",
    "    def tokenize_test(self, row):\n",
    "        text = []\n",
    "        token_map = []\n",
    "        \n",
    "        idx = 0\n",
    "        for t, ws in zip(row[\"tokens\"], row[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            token_map.extend([idx]*len(t))\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                token_map.append(-1)\n",
    "                \n",
    "            idx += 1\n",
    "            \n",
    "        tokenized = self.tokenizer(\"\".join(text), **Config.tokenize_options)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized.input_ids,\n",
    "            \"attention_mask\": tokenized.attention_mask,\n",
    "            \"offset_mapping\": tokenized.offset_mapping,\n",
    "            \"token_map\": token_map,       \n",
    "            \"document\": row[\"document\"],\n",
    "            \"tokens\": row[\"tokens\"],         \n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            row = self.tokenized_df.drop([\"document\", \"tokens\"], axis=1).iloc[index]\n",
    "        else:\n",
    "            row = self.tokenized_df.drop([\"document\", \"tokens\", \"raw_labels\"], axis=1).iloc[index]\n",
    "        return row\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    DataFrameからモデリング時に使用するDataModuleを作成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        valid_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = Config.batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collator = DataCollatorForTokenClassification(\n",
    "            tokenizer, pad_to_multiple_of=512\n",
    "        )\n",
    "        self.reference_df = self.create_val_reference_df(valid_df)\n",
    "\n",
    "    def create_val_reference_df(self, valid_df: pd.DataFrame):\n",
    "        valid_df = valid_df[['document', 'tokens', 'labels']].copy()\n",
    "        valid_df = valid_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n",
    "        valid_df['token'] = valid_df.groupby('document').cumcount()\n",
    "        \n",
    "        reference_df = valid_df[valid_df['label'] != 'O'].copy()\n",
    "        reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n",
    "        reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n",
    "        return reference_df\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CreateDataset(\n",
    "            self.train_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.valid_dataset = CreateDataset(\n",
    "            self.valid_df, self.tokenizer, is_test=False\n",
    "        )\n",
    "        self.test_dataset = CreateDataset(\n",
    "            self.test_df, self.tokenizer, is_test=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            collate_fn=self.collator,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dfs():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    # Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "    train_df, valid_df = downsample_df(df.copy())\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    valid_df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Number of train_df = {len(train_df)}\")\n",
    "    print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "\n",
    "    test_df = pd.read_json(Path(Config.test_path))\n",
    "    clear_memory()\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "more data = 2000\n",
      "pii_dataset_fixed = 4434\n",
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7203 true_others = 166\n",
      "false_samples = 5738 false_others = 134\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df, test_df = gen_dfs()\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_path)\n",
    "dm = CreateDataModule(train_df, valid_df, test_df, tokenizer)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_preds(logits_list: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    preds_final = []\n",
    "    for logits in logits_list:\n",
    "        predictions_prob = torch.softmax(logits, dim=-1)\n",
    "        predictions = logits.argmax(-1)\n",
    "        predictions_without_O = predictions_prob[:, :12].argmax(-1)\n",
    "\n",
    "        O_prob = predictions_prob[:, 12]\n",
    "        pred_final = torch.where(\n",
    "            O_prob < Config.threshold, predictions_without_O, predictions\n",
    "        )\n",
    "        preds_final.append(pred_final)\n",
    "\n",
    "    return preds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_df(preds_list: list[torch.Tensor], tokenized_df: pd.DataFrame):\n",
    "    triplets = []\n",
    "    pairs = set()\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for preds, token_map, offsets, tokens, doc in zip(\n",
    "        preds_list,\n",
    "        tokenized_df[\"token_map\"],\n",
    "        tokenized_df[\"offset_mapping\"],\n",
    "        tokenized_df[\"tokens\"],\n",
    "        tokenized_df[\"document\"],\n",
    "    ):\n",
    "        # p = p.argmax(-1).cpu().detach().numpy()\n",
    "        preds = preds.cpu().detach().numpy()\n",
    "\n",
    "        for token_pred, (start_idx, end_idx) in zip(preds, offsets):\n",
    "            label_pred = Config.id2label[(token_pred)]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            if label_pred == \"O\" or token_id == -1:\n",
    "                continue\n",
    "\n",
    "            pair = (doc, token_id)\n",
    "\n",
    "            if pair in pairs:\n",
    "                continue\n",
    "\n",
    "            document.append(doc)\n",
    "            token.append(token_id)\n",
    "            label.append(label_pred)\n",
    "            token_str.append(tokens[token_id])\n",
    "            pairs.add(pair)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": document, \"token\": token, \"label\": label, \"token_str\": token_str}\n",
    "    )\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module: nn.Module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "class LSTMHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (_, _) = self.lstm.forward(x)\n",
    "        out = hidden\n",
    "        return out\n",
    "\n",
    "\n",
    "class DebertaLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, dm: CreateDataModule):\n",
    "        super(DebertaLSTMModel, self).__init__()\n",
    "\n",
    "        # self.example_input_array = torch.Tensor(1, 1).to(dtype=torch.int64)\n",
    "        self.dm = dm\n",
    "\n",
    "        self.model_config: DebertaV2Config = AutoConfig.from_pretrained(\n",
    "            Config.model_path\n",
    "        )\n",
    "\n",
    "        self.model_config.update(Config.deberta_options)\n",
    "\n",
    "        self.transformers_model: DebertaV2Model = AutoModel.from_pretrained(\n",
    "            Config.model_path, config=self.model_config\n",
    "        )\n",
    "        self.transformers_model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.head = LSTMHead(\n",
    "            in_features=self.model_config.hidden_size,\n",
    "            hidden_dim=self.model_config.hidden_size // 2,\n",
    "            n_layers=1,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.model_config.hidden_size, len(Config.all_labels))\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='mean',ignore_index=-100) \n",
    "        self.validation_step_outputs = []        \n",
    "\n",
    "        # if Config.freeze_layers>0:\n",
    "        # \tprint(f'Freezing {Config.freeze_layers} layers.')\n",
    "        # \tfor layer in self.transformers_model.longformer.encoder.layer[:Config.freeze_layers]:\n",
    "        # \t\tfor param in layer.parameters():\n",
    "        # \t\t\tparam.requires_grad = False\n",
    "        freeze(self.transformers_model.embeddings)\n",
    "        freeze(self.transformers_model.encoder.layer[:2])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, train=True):\n",
    "        transformer_out: BaseModelOutput = self.transformers_model.forward(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = transformer_out.last_hidden_state\n",
    "        head_output = self.head.forward(last_hidden_state)\n",
    "        # last_hidden_state = (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        logits = self.fc.forward(head_output)\n",
    "\n",
    "        # logits = (batch_size, seq_len, num_labels)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        logits = self.forward(input_ids, attention_mask, train=True)\n",
    "        # output = (seq_len, num_labels)\n",
    "        loss = self.loss_function(\n",
    "            logits.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def train_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        print(f\"epoch {self.trainer.current_epoch} training loss {avg_loss}\")\n",
    "        return {\"train_loss\": avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids: torch.Tensor = batch[\"input_ids\"]\n",
    "        attention_mask: torch.Tensor = batch[\"attention_mask\"]\n",
    "        target: torch.Tensor = batch[\"labels\"]\n",
    "\n",
    "        logits = self.forward(input_ids, attention_mask, train=False)\n",
    "        # logits.shape = (batch_size, seq_len, num_labels)\n",
    "\n",
    "        loss = self.loss_function(\n",
    "            logits.view(-1, len(Config.all_labels)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.validation_step_outputs.append(\n",
    "            {\"val_loss\": loss, \"logits\": logits, \"targets\": target}\n",
    "        )\n",
    "        return {\"val_loss\": loss, \"logits\": logits, \"targets\": target}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        logits_list = [logits for batch in outputs for logits in batch[\"logits\"]]\n",
    "        preds_list = post_processing_preds(logits_list)\n",
    "        \n",
    "        pred_df = predictions_to_df(preds_list, self.dm.valid_dataset.tokenized_df)\n",
    "\n",
    "        self.validation_step_outputs = []    \n",
    "\n",
    "        avg_score = compute_metrics(pred_df, self.dm.reference_df)\n",
    "        f5_score = avg_score[\"ents_f5\"]\n",
    "        self.log(\"precision\", avg_score[\"ents_p\"])\n",
    "        self.log(\"recall\", avg_score[\"ents_r\"])\n",
    "        self.log(\"f5\", avg_score[\"ents_f5\"])\n",
    "        \n",
    "        print(f\"epoch {self.trainer.current_epoch} validation loss {avg_loss}\")\n",
    "        print(avg_score[\"ents_per_type\"])\n",
    "\n",
    "        return {\"val_loss\": avg_loss, \"val_f5\": f5_score}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids: torch.Tensor = batch[\"input_ids\"]\n",
    "        attention_mask: torch.Tensor = batch[\"attention_mask\"]\n",
    "\n",
    "        logits = self.forward(input_ids, attention_mask, train=False)\n",
    "        return logits\n",
    "\n",
    "    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.transformers_model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.named_parameters()\n",
    "                    if \"transformers_model\" not in n\n",
    "                ],\n",
    "                \"lr\": decoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=Config.lr)\n",
    "\n",
    "        epoch_steps = len(self.dm.train_dataset)\n",
    "        batch_size = Config.batch_size\n",
    "\n",
    "        warmup_steps = 0.05 * epoch_steps // batch_size\n",
    "        training_steps = Config.epochs * epoch_steps // batch_size\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n",
    "        # scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=1e-6, power=3.0)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, warmup_steps, training_steps, num_cycles=0.5\n",
    "        )\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhzume\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/scripts/exp002/wandb/run-20240327_232546-ihgylmpw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hzume/pii-002-002/runs/ihgylmpw/workspace' target=\"_blank\">legendary-disco-9</a></strong> to <a href='https://wandb.ai/hzume/pii-002-002' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hzume/pii-002-002' target=\"_blank\">https://wandb.ai/hzume/pii-002-002</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hzume/pii-002-002/runs/ihgylmpw/workspace' target=\"_blank\">https://wandb.ai/hzume/pii-002-002/runs/ihgylmpw/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:398: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /kaggle/input/pii-002-002 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7ff1d770cd4adc8c0eb8f27c4d23bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss 2.8252460956573486\n",
      "{'PHONE_NUM': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'STREET_ADDRESS': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'ID_NUM': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'NAME_STUDENT': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'EMAIL': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'URL_PERSONAL': {'p': 0.0, 'r': 0.0, 'f5': 0.0}, 'USERNAME': {'p': 0.0, 'r': 0.0, 'f5': 0.0}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d3a37c2f594067be409d68e811ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f29adcbe7f34f2cb1d90b0fa8311922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss 0.06602268666028976\n",
      "{'NAME_STUDENT': {'p': 0.16153028692879914, 'r': 0.7272727272727273, 'f5': 0.6409341550437885}, 'STREET_ADDRESS': {'p': 0.7099903006789525, 'r': 0.814238042269188, 'f5': 0.8096656172892027}, 'EMAIL': {'p': 0.191025641025641, 'r': 1.0, 'f5': 0.8599334073251943}, 'PHONE_NUM': {'p': 0.5420054200542005, 'r': 0.7736943907156673, 'f5': 0.7611798287345385}, 'URL_PERSONAL': {'p': 0.2791666666666667, 'r': 0.9852941176470589, 'f5': 0.8979381443298969}, 'ID_NUM': {'p': 0.1724137931034483, 'r': 0.03816793893129771, 'f5': 0.03934624697336562}, 'USERNAME': {'p': 0.0, 'r': 0.0, 'f5': 0.0}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7f7a497bb143b395be049ac42ef054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss 0.028016192838549614\n",
      "{'NAME_STUDENT': {'p': 0.44482758620689655, 'r': 0.8641148325358852, 'f5': 0.8338838572189664}, 'ID_NUM': {'p': 0.30994152046783624, 'r': 0.40458015267175573, 'f5': 0.39988392338943707}, 'STREET_ADDRESS': {'p': 0.7705357142857143, 'r': 0.9599555061179088, 'f5': 0.9509641873278237}, 'URL_PERSONAL': {'p': 0.2518796992481203, 'r': 0.9852941176470589, 'f5': 0.8860630722278738}, 'USERNAME': {'p': 0.461038961038961, 'r': 0.9726027397260274, 'f5': 0.9327943405760484}, 'PHONE_NUM': {'p': 0.6234413965087282, 'r': 0.9671179883945842, 'f5': 0.9470386828877395}, 'EMAIL': {'p': 0.2811320754716981, 'r': 1.0, 'f5': 0.9104582843713279}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0bc0927be44733933dfc37841b3149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss 0.016700921580195427\n",
      "{'PHONE_NUM': {'p': 0.6953551912568307, 'r': 0.9845261121856866, 'f5': 0.969026872666032}, 'STREET_ADDRESS': {'p': 0.7856502242152467, 'r': 0.9744160177975528, 'f5': 0.9654938533276813}, 'ID_NUM': {'p': 0.29767441860465116, 'r': 0.48854961832061067, 'f5': 0.4767908309455587}, 'NAME_STUDENT': {'p': 0.8171788810086682, 'r': 0.9923444976076555, 'f5': 0.984230123384683}, 'USERNAME': {'p': 0.5, 'r': 0.9863013698630136, 'f5': 0.9507364144235653}, 'EMAIL': {'p': 0.4209039548022599, 'r': 1.0, 'f5': 0.9497425839666586}, 'URL_PERSONAL': {'p': 0.3316831683168317, 'r': 0.9852941176470589, 'f5': 0.9158780231335437}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f230b1b79d404592f18e41960aafc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss 0.012623980641365051\n",
      "{'PHONE_NUM': {'p': 0.7818181818181819, 'r': 0.9980657640232108, 'f5': 0.9875598086124402}, 'STREET_ADDRESS': {'p': 0.8818818818818819, 'r': 0.9799777530589544, 'f5': 0.9758030161029224}, 'NAME_STUDENT': {'p': 0.7780269058295964, 'r': 0.9961722488038277, 'f5': 0.985544186723956}, 'ID_NUM': {'p': 0.695906432748538, 'r': 0.9083969465648855, 'f5': 0.8978525827045849}, 'USERNAME': {'p': 0.6486486486486487, 'r': 0.9863013698630136, 'f5': 0.9669421487603305}, 'EMAIL': {'p': 0.7061611374407583, 'r': 1.0, 'f5': 0.9842479674796747}, 'URL_PERSONAL': {'p': 0.6868686868686869, 'r': 1.0, 'f5': 0.9827682045580879}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fbe7fed1d3401d88742b3c8f3b2b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.011635947972536087\n",
      "{'PHONE_NUM': {'p': 0.9086115992970123, 'r': 1.0, 'f5': 0.9961464354527938}, 'STREET_ADDRESS': {'p': 0.9534632034632035, 'r': 0.9799777530589544, 'f5': 0.9789307235351938}, 'NAME_STUDENT': {'p': 0.86511240632806, 'r': 0.9942583732057416, 'f5': 0.9885823025689818}, 'ID_NUM': {'p': 0.8223684210526315, 'r': 0.9541984732824428, 'f5': 0.9483513276918589}, 'USERNAME': {'p': 0.7272727272727273, 'r': 0.9863013698630136, 'f5': 0.9729729729729729}, 'EMAIL': {'p': 0.7801047120418848, 'r': 1.0, 'f5': 0.9892747701736466}, 'URL_PERSONAL': {'p': 0.7640449438202247, 'r': 1.0, 'f5': 0.9882615986584685}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c384af98b5446aaebce9940c41ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.009751223027706146\n",
      "{'PHONE_NUM': {'p': 0.9265232974910395, 'r': 1.0, 'f5': 0.9969591337239487}, 'STREET_ADDRESS': {'p': 0.9660087719298246, 'r': 0.9799777530589544, 'f5': 0.979433018343524}, 'NAME_STUDENT': {'p': 0.8552631578947368, 'r': 0.9952153110047847, 'f5': 0.9889908927983614}, 'ID_NUM': {'p': 0.8888888888888888, 'r': 0.9770992366412213, 'f5': 0.9733840304182508}, 'USERNAME': {'p': 0.7578947368421053, 'r': 0.9863013698630136, 'f5': 0.9749999999999999}, 'EMAIL': {'p': 0.7925531914893617, 'r': 1.0, 'f5': 0.9900332225913621}, 'URL_PERSONAL': {'p': 0.8395061728395061, 'r': 1.0, 'f5': 0.9927007299270073}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c758d76e393b452ba2b8b899d0aedbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.008794000372290611\n",
      "{'PHONE_NUM': {'p': 0.955637707948244, 'r': 1.0, 'f5': 0.9982177335511659}, 'STREET_ADDRESS': {'p': 0.9876681614349776, 'r': 0.9799777530589544, 'f5': 0.9802713228056662}, 'NAME_STUDENT': {'p': 0.8705096073517126, 'r': 0.9971291866028709, 'f5': 0.9915818754117562}, 'ID_NUM': {'p': 0.9071428571428571, 'r': 0.9694656488549618, 'f5': 0.9669106881405564}, 'USERNAME': {'p': 0.8372093023255814, 'r': 0.9863013698630136, 'f5': 0.9795918367346937}, 'EMAIL': {'p': 0.8186813186813187, 'r': 1.0, 'f5': 0.9915536217046327}, 'URL_PERSONAL': {'p': 0.85, 'r': 1.0, 'f5': 0.9932584269662921}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05834b7e928f45f8b4476bc3054faa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation loss 0.008602716960012913\n",
      "{'PHONE_NUM': {'p': 0.955637707948244, 'r': 1.0, 'f5': 0.9982177335511659}, 'STREET_ADDRESS': {'p': 0.9876681614349776, 'r': 0.9799777530589544, 'f5': 0.9802713228056662}, 'NAME_STUDENT': {'p': 0.853518821603928, 'r': 0.9980861244019139, 'f5': 0.9916261381504369}, 'ID_NUM': {'p': 0.8936170212765957, 'r': 0.9618320610687023, 'f5': 0.9590163934426231}, 'USERNAME': {'p': 0.8674698795180723, 'r': 0.9863013698630136, 'f5': 0.9811320754716981}, 'EMAIL': {'p': 0.8662790697674418, 'r': 1.0, 'f5': 0.9940980241211187}, 'URL_PERSONAL': {'p': 0.8717948717948718, 'r': 1.0, 'f5': 0.9943757030371204}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b48db6ccc8e4896b26ac6f5581ecbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation loss 0.009531003423035145\n",
      "{'PHONE_NUM': {'p': 0.9451553930530164, 'r': 1.0, 'f5': 0.9977731591448932}, 'STREET_ADDRESS': {'p': 0.8127306273062731, 'r': 0.9799777530589544, 'f5': 0.9722823549386647}, 'NAME_STUDENT': {'p': 0.7762863534675615, 'r': 0.9961722488038277, 'f5': 0.9854365397218379}, 'ID_NUM': {'p': 0.7678571428571429, 'r': 0.9847328244274809, 'f5': 0.9741504501887888}, 'USERNAME': {'p': 0.7578947368421053, 'r': 0.9863013698630136, 'f5': 0.9749999999999999}, 'EMAIL': {'p': 0.8514285714285714, 'r': 1.0, 'f5': 0.9933333333333333}, 'URL_PERSONAL': {'p': 0.8095238095238095, 'r': 1.0, 'f5': 0.9910313901345292}}\n"
     ]
    }
   ],
   "source": [
    "if Config.training:\n",
    "    wandb.login()\n",
    "    id = wandb.util.generate_id()\n",
    "    wandb.init(project=Config.project_name, id=id, resume=\"allow\")\n",
    "    # if Config.resume:\n",
    "    #     wandb.init(project=f'pii-{Config.exp}-{Config.ver}', id=\"helpful-water-6\")\n",
    "    wandb_logger = WandbLogger(project=Config.project_name)\n",
    "    model = DebertaLSTMModel(dm=dm)\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=0.00, patience=8, verbose=True, mode=\"min\"\n",
    "    )\n",
    "    Path(Config.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(Config.save_dir).joinpath(\"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"title\": Config.project_name,\n",
    "                \"id\": f\"zume666/{Config.project_name}\",\n",
    "                \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=Config.save_dir,\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "        save_weights_only=False,\n",
    "        filename=Config.model_name,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=Config.epochs,\n",
    "        deterministic=True,\n",
    "        val_check_interval=0.25,\n",
    "        accumulate_grad_batches=4,\n",
    "        devices=[0],\n",
    "        precision=\"bf16-mixed\",\n",
    "        accelerator=\"gpu\",\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            early_stop_callback,\n",
    "            ModelSummary(max_depth=-1),\n",
    "        ],\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "    if Config.resume:\n",
    "        trainer.fit(\n",
    "            model=model,\n",
    "            datamodule=dm,\n",
    "            ckpt_path=Path(Config.save_dir).joinpath(\"last.ckpt\"),\n",
    "        )\n",
    "    else:\n",
    "        trainer.fit(model=model, datamodule=dm)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dm: CreateDataModule, model: pl.LightningModule):\n",
    "    model.eval()  \n",
    "    model.to(DEVICE)\n",
    "    test_dataloader = dm.test_dataloader()\n",
    "    \n",
    "    trainer = pl.Trainer()\n",
    "    outputs = trainer.predict(model=model, dataloaders=test_dataloader, ckpt_path=Config.save_path)\n",
    "        \n",
    "    logits_list = [logits for batch in outputs for logits in batch]\n",
    "    preds_list = post_processing_preds(logits_list)\n",
    "\n",
    "    pred_df = predictions_to_df(preds_list, dm.test_dataset.tokenized_df)\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13122bead7eb4548a3a28001fb5e8f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ~Config.training:\n",
    "    model = DebertaLSTMModel(dm=dm)\n",
    "    sub_df = predict(dm, model)\n",
    "    sample_sub = pd.read_csv(Config.sample_sub_path)\n",
    "    sub_df = sub_df[sample_sub.columns]\n",
    "    sub_df.to_csv('submission.csv',index=False)\n",
    "    display(sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert preds to a list of dictionaries\n",
    "# results = []\n",
    "# for preds, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "#                                               tokenized_ds[\"token_map\"], \n",
    "#                                               tokenized_ds[\"offset_mapping\"],\n",
    "#                                               tokenized_ds[\"tokens\"],\n",
    "#                                               tokenized_ds[\"document\"]):\n",
    "#     for token_pred, (start_idx, end_idx) in zip(preds, offsets):\n",
    "#         try:\n",
    "#             label_pred = Config.id2label[str(token_pred)]\n",
    "#             if start_idx + end_idx == 0: \n",
    "#                 continue\n",
    "\n",
    "#             if token_map[start_idx] == -1:\n",
    "#                 start_idx += 1\n",
    "#              # ignore \"\\n\\n\"\n",
    "#             while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "#                 start_idx += 1\n",
    "\n",
    "#             if start_idx >= len(token_map): \n",
    "#                 break\n",
    "\n",
    "#             token_id = token_map[start_idx]\n",
    "\n",
    "#             # ignore \"O\" predictions and whitespace preds\n",
    "#             if label_pred != \"O\" and token_id != -1:\n",
    "#                 results.append({\n",
    "#                         \"document\": doc,\n",
    "#                         \"token\": token_id,\n",
    "#                         \"label\": label_pred,\n",
    "#                         \"token_str\": tokens[token_id]\n",
    "#                     })\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error {e}\")\n",
    "#             print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "#             sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "\n",
    "# def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "#     idx = 0\n",
    "#     spans = []\n",
    "#     span = []\n",
    "    \n",
    "#     for i, token in enumerate(document):\n",
    "#         if token != target[idx]:\n",
    "#             idx = 0\n",
    "#             span = []\n",
    "#             continue\n",
    "#         span.append(i)\n",
    "        \n",
    "#         idx += 1\n",
    "#         if idx == len(target):\n",
    "#             spans.append(span)\n",
    "#             span = []\n",
    "#             idx = 0\n",
    "#             continue\n",
    "    \n",
    "#     return spans\n",
    "\n",
    "# email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "# phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "# emails = []\n",
    "# phone_nums = []\n",
    "\n",
    "# for _data in test_ds:\n",
    "#     # email\n",
    "#     for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "#         if re.fullmatch(email_regex, token) is not None:\n",
    "#             emails.append(\n",
    "#                 {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "#             )\n",
    "#     # phone number\n",
    "#     matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "#     if not matches:\n",
    "#         continue\n",
    "        \n",
    "#     for match in matches:\n",
    "#         target = [t.text for t in nlp.tokenizer(match)]\n",
    "#         matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "#     for matched_span in matched_spans:\n",
    "#         for intermediate, token_idx in enumerate(matched_span):\n",
    "#             prefix = \"I\" if intermediate else \"B\"\n",
    "#             phone_nums.append(\n",
    "#                 {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "#             )\n",
    "\n",
    "# results.extend(emails)\n",
    "# results.extend(phone_nums)\n",
    "\n",
    "# def remove_duplicates(df: pd.DataFrame):\n",
    "#     # Sort by the document and token\n",
    "#     df.sort_values(by=['document', 'token'])\n",
    "#     # Combine three columns \n",
    "#     df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "#     # display(df)\n",
    "#     # Drop duplicated triplets and keep the first one as unique row\n",
    "#     df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "#     # Regenerate 'row_id'\n",
    "#     df['row_id'] = list(range(len(df)))    \n",
    "#     df = df.reset_index(drop=True, inplace=False) \n",
    "#     print(\"Remove duplicates\")\n",
    "# #     display(df)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame(results)\n",
    "# test_df = remove_duplicates(test_df)\n",
    "# test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# # Create submission df\n",
    "# test_df.to_csv(\"submission.csv\", index=False)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
