{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-small\"\n",
    "    downsample = 0.45\n",
    "    truncation = True\n",
    "    padding = False  #'max_length'\n",
    "    max_length = 1024\n",
    "    freeze_layers = 0\n",
    "\n",
    "    target_cols = [\n",
    "        \"B-EMAIL\",\n",
    "        \"B-ID_NUM\",\n",
    "        \"B-NAME_STUDENT\",\n",
    "        \"B-PHONE_NUM\",\n",
    "        \"B-STREET_ADDRESS\",\n",
    "        \"B-URL_PERSONAL\",\n",
    "        \"B-USERNAME\",\n",
    "        \"I-ID_NUM\",\n",
    "        \"I-NAME_STUDENT\",\n",
    "        \"I-PHONE_NUM\",\n",
    "        \"I-STREET_ADDRESS\",\n",
    "        \"I-URL_PERSONAL\",\n",
    "        \"O\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DebertaV2Config,\n",
    "    DebertaV2ForTokenClassification,\n",
    ")\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    # more_data = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\")\n",
    "    # print(f\"more data = {len(more_data)}\")\n",
    "    # pii_dataset_fixed = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\")\n",
    "    # print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    # df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    # df = train_data\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "\n",
    "    return df, all_labels, label2id, id2label\n",
    "\n",
    "\n",
    "# Eencode labels to columns\n",
    "def encode_labels(df: pd.DataFrame):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "all_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n",
      "EMAIL: 5661  (42.8%)\n",
      "ID_NUM: 1865  (14.1%)\n",
      "NAME_STUDENT: 6834  (51.6%)\n",
      "PHONE_NUM: 4214  (31.8%)\n",
      "STREET_ADDRESS: 5116  (38.6%)\n",
      "URL_PERSONAL: 2546  (19.2%)\n",
      "USERNAME: 2513  (19.0%)\n",
      "others: 5872  (44.3%)\n"
     ]
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data()\n",
    "df_labels, label_classes = encode_labels(df.copy())\n",
    "# df_labels.to_csv(\"df_labels.csv\", encoding=\"utf-8\")\n",
    "# display(df_labels.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples = len(true_labels) - 150\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - 150\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7219 true_others = 150\n",
      "false_samples = 5722 false_others = 150\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    }
   ],
   "source": [
    "# Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "train_df, valid_df = downsample_df(df.copy())\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(example: pd.DataFrame, tokenizer: AutoTokenizer, label2id: dict[str, int]):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and\n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O'\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            label_id = label2id[labels[start_idx]]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df: pd.DataFrame):\n",
    "        ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"full_text\": df[\"full_text\"].tolist(),\n",
    "                \"document\": df[\"document\"].astype(\"string\"),\n",
    "                \"tokens\": df[\"tokens\"].tolist(),\n",
    "                \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "                \"provided_labels\": df[\"labels\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "        # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(\n",
    "            tokenize,\n",
    "            fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
    "            num_proc=4,\n",
    "        )\n",
    "        return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec950a17ab114e08a03d751c694019f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3311 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241e12416bdb4ff1b0be1e8b25c28f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b5a3fd37704619bb5636cc980bbd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebb415b4641477e98ec1afeab7ac415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = create_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens(example: dict[str, Any]):\n",
    "    end_token = [\"\\n\\n\", \".\", \"!\", \"?\", \";\"]\n",
    "    tokens = example[\"tokens\"]\n",
    "    ret = []\n",
    "    sub_tokens = []\n",
    "    for token in tokens:\n",
    "        sub_tokens.append(token)\n",
    "        if token in end_token:\n",
    "            if len(sub_tokens) > 500:\n",
    "                print(sub_tokens)\n",
    "            ret.append(len(sub_tokens))\n",
    "            sub_tokens = []\n",
    "    example[\"max_length_splited_tokens\"] = max(ret)\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457795fb60ab46dfa0674c5a3886bb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3311 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d4975d3ebc446ca8c60f1d5b7d2f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb1a26641e5497da5239505cea7467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b29681953fa4668ac83edc085aa4de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_splited_ds = tokenized_ds.map(\n",
    "    split_tokens,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_splited_df = tokenized_splited_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = tokenized_splited_df[\"max_length_splited_tokens\"]\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(a, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192     247\n",
       "607     209\n",
       "1109    202\n",
       "1694    318\n",
       "1884    404\n",
       "2416    216\n",
       "4205    251\n",
       "5285    239\n",
       "5309    201\n",
       "5520    272\n",
       "6782    482\n",
       "Name: max_length_splited_tokens, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "    \n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.num_proc = 8\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3  # Number of epochs\n",
    "        self.batch_size = 1  # Too large batch sizes lead to OOM\n",
    "        self.fp16 = True if torch.cuda.is_available() else False\n",
    "        self.model_path = Config.model_path\n",
    "        self.output_dir = \"/kaggle/output\"\n",
    "        self.save_path = f\"/kaggle/deberta3base-truncation-false-{EXP}-{VER}\"\n",
    "        self.load_model()\n",
    "\n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        # Load tokenizer Config\n",
    "        Config = DebertaV2Config.from_pretrained(self.model_path)\n",
    "        # Increase context length using the max_position_embeddings parameter\n",
    "        Config.update(\n",
    "            {\n",
    "                \"num_labels\": len(self.all_labels),\n",
    "                \"id2label\": self.id2label,\n",
    "                \"label2id\": self.label2id,\n",
    "                \"ignore_mismatched_sizes\": True,\n",
    "            }\n",
    "        )\n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_path, Config=Config\n",
    "        )\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df: pd.DataFrame):\n",
    "        ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"full_text\": df[\"full_text\"].tolist(),\n",
    "                \"document\": df[\"document\"].astype(\"string\"),\n",
    "                \"tokens\": df[\"tokens\"].tolist(),\n",
    "                \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "                \"provided_labels\": df[\"labels\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "        # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(\n",
    "            tokenize,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"label2id\": self.label2id},\n",
    "            num_proc=self.num_proc,\n",
    "        )\n",
    "        return tokenized_ds\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df):\n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(\n",
    "            self.tokenizer, pad_to_multiple_of=16\n",
    "        )\n",
    "\n",
    "        # Trainer cofiguration\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            fp16=self.fp16,  # # Change to False if using CPU only\n",
    "            learning_rate=self.learning_rate,\n",
    "            num_train_epochs=self.num_train_epochs,  # The total number of training epochs to run.\n",
    "            per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,  # batch size for evaluation\n",
    "            gradient_accumulation_steps=2,\n",
    "            report_to=\"none\",\n",
    "            evaluation_strategy=\"epoch\",  # Evaluated at the end of epochs\n",
    "            # eval_steps=1,\n",
    "            do_eval=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,  # Save the best and most recent checkpoints\n",
    "            logging_steps=20,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,  # Load the best model at the end\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=0.1,  # number of warmup steps (0.1) for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "        )\n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=training_ds,\n",
    "            eval_dataset=valid_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training requires the GPUs and internet\n",
    "TRAINING = True # True: Model Training, False: Model Inference\n",
    "if TRAINING: \n",
    "    # Configuration class containing various model and training parameters\n",
    "    trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "    trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIDE=384\n",
    "MAX_LENGTH = 1024\n",
    "model_path = f\"/kaggle/input/deberta3base-truncation-false-{EXP}-{VER}-small\"\n",
    "threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    tokens = []\n",
    "    token_map = []\n",
    "    idx = 0\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        tokens.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            tokens.append(\" \")\n",
    "            token_map.append(-1) \n",
    "        idx += 1\n",
    "    # Does not truncate the text and concate all words together \n",
    "    # Do not need to have extra space as we have already include it in the previous tokenization\n",
    "    tokenized = tokenizer(\"\".join(tokens), return_offsets_mapping=True, truncation=False)\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from datasets import Dataset \n",
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.model_path = model_path\n",
    "        self.max_length = 1024\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.num_proc = 3 # 3 processors\n",
    "        self.threshold = threshold # Threashold\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)        \n",
    "        # # Load the fine-tuned adapter layer on top of base model\n",
    "        # self.model = self.model.to(DEVICE)\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    def post_processing_preds(self, preds):\n",
    "        preds_final = []\n",
    "        preds_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0],\n",
    "                                                                              preds.shape[1],\n",
    "                                                                              1)\n",
    "        # Get the maximal value as the final preds\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_without_O = preds_softmax[:,:,:12].argmax(-1) # Prob of entity labels (like 'NAME_STUDENT')\n",
    "        O_preds = preds_softmax[:,:,12] # Prob for 'O'\n",
    "        print()\n",
    "        # If preds for 'O' > 0.99, select preds of 'O'. Otherwise, select preds for entity labels.  \n",
    "        preds_final = np.where(O_preds < self.threshold, preds_without_O, preds)\n",
    "        return preds_final     \n",
    "    \n",
    "    def infer_preds(self, ds):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = self.post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "Config = json.load(open(Path(model_path) / \"Config.json\"))\n",
    "id2label = Config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer(all_labels, label2id, id2label)\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        try:\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "             # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "    \n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
