{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = \"001\"\n",
    "VER = \"002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "# Transformer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DebertaV2Config,\n",
    "    DebertaV2ForTokenClassification,\n",
    ")\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    more_data = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\")\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "    pii_dataset_fixed = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\")\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    # df = train_data\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "\n",
    "    return df, all_labels, label2id, id2label\n",
    "\n",
    "\n",
    "# Eencode labels to columns\n",
    "def encode_labels(df: pd.DataFrame):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "more data = 2000\n",
      "pii_dataset_fixed = 4434\n",
      "all_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n",
      "EMAIL: 5661  (42.8%)\n",
      "ID_NUM: 1865  (14.1%)\n",
      "NAME_STUDENT: 6834  (51.6%)\n",
      "PHONE_NUM: 4214  (31.8%)\n",
      "STREET_ADDRESS: 5116  (38.6%)\n",
      "URL_PERSONAL: 2546  (19.2%)\n",
      "USERNAME: 2513  (19.0%)\n",
      "others: 5872  (44.3%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>unique_labels</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>NAME_STUDENT</th>\n",
       "      <th>PHONE_NUM</th>\n",
       "      <th>STREET_ADDRESS</th>\n",
       "      <th>URL_PERSONAL</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>13231</td>\n",
       "      <td>My name is Gaspard Ma. I'm a geologist at 0891...</td>\n",
       "      <td>[My, name, is, Gaspard, Ma, ., I, 'm, a, geolo...</td>\n",
       "      <td>[True, True, True, True, False, True, False, T...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[EMAIL, NAME_STUDENT, STREET_ADDRESS]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13232</th>\n",
       "      <td>13232</td>\n",
       "      <td>In my role as an economist with three years of...</td>\n",
       "      <td>[In, my, role, as, an, economist, with, three,...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[EMAIL, NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13233</th>\n",
       "      <td>13233</td>\n",
       "      <td>My name is Masami Roux. The year was 2019, and...</td>\n",
       "      <td>[My, name, is, Masami, Roux, ., The, year, was...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Tr...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[EMAIL, NAME_STUDENT, PHONE_NUM]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13234</th>\n",
       "      <td>13234</td>\n",
       "      <td>I'm Dr. Kenji Koch, an ophthalmologist with ov...</td>\n",
       "      <td>[I, 'm, Dr., Kenji, Koch, ,, an, ophthalmologi...</td>\n",
       "      <td>[False, True, True, True, False, True, True, T...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[USERNAME, NAME_STUDENT, URL_PERSONAL, STREET_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>13235</td>\n",
       "      <td>My name is Oleg Richter, and I'm a radiologist...</td>\n",
       "      <td>[My, name, is, Oleg, Richter, ,, and, I, 'm, a...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Fa...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>13236</td>\n",
       "      <td>During my 15-year career as a developer, I hav...</td>\n",
       "      <td>[During, my, 15, -, year, career, as, a, devel...</td>\n",
       "      <td>[True, True, False, False, True, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[EMAIL, PHONE_NUM, NAME_STUDENT, STREET_ADDRESS]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>13237</td>\n",
       "      <td>In 2019, a particularly challenging case came ...</td>\n",
       "      <td>[In, 2019, ,, a, particularly, challenging, ca...</td>\n",
       "      <td>[True, False, True, True, True, True, True, Tr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[STREET_ADDRESS]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>13238</td>\n",
       "      <td>Hello there, I'm Krishna Lopez and I've been a...</td>\n",
       "      <td>[Hello, there, ,, I, 'm, Krishna, Lopez, and, ...</td>\n",
       "      <td>[True, False, True, False, True, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "      <td>[USERNAME, NAME_STUDENT, URL_PERSONAL, STREET_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>13239</td>\n",
       "      <td>As a designer, I've encountered various challe...</td>\n",
       "      <td>[As, a, designer, ,, I, 've, encountered, vari...</td>\n",
       "      <td>[True, True, False, True, False, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[EMAIL, NAME_STUDENT, STREET_ADDRESS]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13240</th>\n",
       "      <td>13240</td>\n",
       "      <td>Hello, my name is Hiroko Yu and I am a freelan...</td>\n",
       "      <td>[Hello, ,, my, name, is, Hiroko, Yu, and, I, a...</td>\n",
       "      <td>[False, True, True, True, True, True, True, Tr...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "      <td>[EMAIL, NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       document                                          full_text  \\\n",
       "13231     13231  My name is Gaspard Ma. I'm a geologist at 0891...   \n",
       "13232     13232  In my role as an economist with three years of...   \n",
       "13233     13233  My name is Masami Roux. The year was 2019, and...   \n",
       "13234     13234  I'm Dr. Kenji Koch, an ophthalmologist with ov...   \n",
       "13235     13235  My name is Oleg Richter, and I'm a radiologist...   \n",
       "13236     13236  During my 15-year career as a developer, I hav...   \n",
       "13237     13237  In 2019, a particularly challenging case came ...   \n",
       "13238     13238  Hello there, I'm Krishna Lopez and I've been a...   \n",
       "13239     13239  As a designer, I've encountered various challe...   \n",
       "13240     13240  Hello, my name is Hiroko Yu and I am a freelan...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "13231  [My, name, is, Gaspard, Ma, ., I, 'm, a, geolo...   \n",
       "13232  [In, my, role, as, an, economist, with, three,...   \n",
       "13233  [My, name, is, Masami, Roux, ., The, year, was...   \n",
       "13234  [I, 'm, Dr., Kenji, Koch, ,, an, ophthalmologi...   \n",
       "13235  [My, name, is, Oleg, Richter, ,, and, I, 'm, a...   \n",
       "13236  [During, my, 15, -, year, career, as, a, devel...   \n",
       "13237  [In, 2019, ,, a, particularly, challenging, ca...   \n",
       "13238  [Hello, there, ,, I, 'm, Krishna, Lopez, and, ...   \n",
       "13239  [As, a, designer, ,, I, 've, encountered, vari...   \n",
       "13240  [Hello, ,, my, name, is, Hiroko, Yu, and, I, a...   \n",
       "\n",
       "                                     trailing_whitespace  \\\n",
       "13231  [True, True, True, True, False, True, False, T...   \n",
       "13232  [True, True, True, True, True, True, True, Tru...   \n",
       "13233  [True, True, True, True, False, True, True, Tr...   \n",
       "13234  [False, True, True, True, False, True, True, T...   \n",
       "13235  [True, True, True, True, False, True, True, Fa...   \n",
       "13236  [True, True, False, False, True, True, True, T...   \n",
       "13237  [True, False, True, True, True, True, True, Tr...   \n",
       "13238  [True, False, True, False, True, True, True, T...   \n",
       "13239  [True, True, False, True, False, True, True, T...   \n",
       "13240  [False, True, True, True, True, True, True, Tr...   \n",
       "\n",
       "                                                  labels  \\\n",
       "13231  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13232  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13233  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13234  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13235  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13236  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13237  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13238  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...   \n",
       "13239  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13240  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...   \n",
       "\n",
       "                                           unique_labels  EMAIL  ID_NUM  \\\n",
       "13231              [EMAIL, NAME_STUDENT, STREET_ADDRESS]      1       0   \n",
       "13232   [EMAIL, NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]      1       0   \n",
       "13233                   [EMAIL, NAME_STUDENT, PHONE_NUM]      1       0   \n",
       "13234  [USERNAME, NAME_STUDENT, URL_PERSONAL, STREET_...      0       0   \n",
       "13235          [NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]      0       0   \n",
       "13236   [EMAIL, PHONE_NUM, NAME_STUDENT, STREET_ADDRESS]      1       0   \n",
       "13237                                   [STREET_ADDRESS]      0       0   \n",
       "13238  [USERNAME, NAME_STUDENT, URL_PERSONAL, STREET_...      0       0   \n",
       "13239              [EMAIL, NAME_STUDENT, STREET_ADDRESS]      1       0   \n",
       "13240   [EMAIL, NAME_STUDENT, PHONE_NUM, STREET_ADDRESS]      1       0   \n",
       "\n",
       "       NAME_STUDENT  PHONE_NUM  STREET_ADDRESS  URL_PERSONAL  USERNAME  others  \n",
       "13231             1          0               1             0         0       0  \n",
       "13232             1          1               1             0         0       0  \n",
       "13233             1          1               0             0         0       0  \n",
       "13234             1          0               1             1         1       0  \n",
       "13235             1          1               1             0         0       0  \n",
       "13236             1          1               1             0         0       0  \n",
       "13237             0          0               1             0         0       0  \n",
       "13238             1          0               1             1         1       0  \n",
       "13239             1          0               1             0         0       0  \n",
       "13240             1          1               1             0         0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data()\n",
    "df_labels, label_classes = encode_labels(df.copy())\n",
    "df_labels.to_csv(\"df_labels.csv\", encoding=\"utf-8\")\n",
    "display(df_labels.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples = len(true_labels) - 150\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - 150\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7219 true_others = 150\n",
      "false_samples = 5722 false_others = 150\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    }
   ],
   "source": [
    "# Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "train_df, valid_df = downsample_df(df.copy())\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(example: pd.DataFrame, tokenizer: AutoTokenizer, label2id: dict[str, int]):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and\n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    # tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False)\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=False, max_length=384, stride=32, return_overflowing_tokens=True)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O'\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            label_id = label2id[labels[start_idx]]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"full_text\": df[\"full_text\"].tolist(),\n",
    "                \"document\": df[\"document\"].astype(\"string\"),\n",
    "                \"tokens\": df[\"tokens\"].tolist(),\n",
    "                \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "                \"provided_labels\": df[\"labels\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/huggingfacedebertav3variants/deberta-v3-small\")\n",
    "# tokenized_ds = ds.map(\n",
    "#             tokenize,\n",
    "#             fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id},\n",
    "#             num_proc=4,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "a_i = tokenizer(ds[i][\"full_text\"], return_offsets_mapping=True, truncation=True, max_length=384, stride=128, return_overflowing_tokens=True)\n",
    "len(a_i[\"input_ids\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "    \n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.num_proc = 8\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3  # Number of epochs\n",
    "        self.batch_size = 1  # Too large batch sizes lead to OOM\n",
    "        self.fp16 = True if torch.cuda.is_available() else False\n",
    "        self.model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-small\"\n",
    "        self.output_dir = \"/kaggle/output\"\n",
    "        self.save_path = f\"/kaggle/deberta3base-truncation-false-{EXP}-{VER}\"\n",
    "        self.load_model()\n",
    "\n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        # Load tokenizer config\n",
    "        config = DebertaV2Config.from_pretrained(self.model_path)\n",
    "        # Increase context length using the max_position_embeddings parameter\n",
    "        config.update(\n",
    "            {\n",
    "                \"num_labels\": len(self.all_labels),\n",
    "                \"id2label\": self.id2label,\n",
    "                \"label2id\": self.label2id,\n",
    "                \"ignore_mismatched_sizes\": True,\n",
    "            }\n",
    "        )\n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_path, config=config\n",
    "        )\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df: pd.DataFrame):\n",
    "        ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"full_text\": df[\"full_text\"].tolist(),\n",
    "                \"document\": df[\"document\"].astype(\"string\"),\n",
    "                \"tokens\": df[\"tokens\"].tolist(),\n",
    "                \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "                \"provided_labels\": df[\"labels\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "        # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(\n",
    "            tokenize,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"label2id\": self.label2id},\n",
    "            num_proc=self.num_proc,\n",
    "        )\n",
    "        return tokenized_ds\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df):\n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(\n",
    "            self.tokenizer, pad_to_multiple_of=16\n",
    "        )\n",
    "\n",
    "        # Trainer cofiguration\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            fp16=self.fp16,  # # Change to False if using CPU only\n",
    "            learning_rate=self.learning_rate,\n",
    "            num_train_epochs=self.num_train_epochs,  # The total number of training epochs to run.\n",
    "            per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,  # batch size for evaluation\n",
    "            gradient_accumulation_steps=2,\n",
    "            report_to=\"none\",\n",
    "            evaluation_strategy=\"epoch\",  # Evaluated at the end of epochs\n",
    "            # eval_steps=1,\n",
    "            do_eval=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,  # Save the best and most recent checkpoints\n",
    "            logging_steps=20,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,  # Load the best model at the end\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=0.1,  # number of warmup steps (0.1) for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "        )\n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=training_ds,\n",
    "            eval_dataset=valid_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at /kaggle/input/huggingfacedebertav3variants/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading pretrained LLM model\n",
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5242c71454048eaa6d25ebe69453e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1618 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 520, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 487, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py\", line 458, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2320, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2220, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1915, in decorated\n    result = f(decorated_item, *args, **kwargs)\n  File \"/tmp/ipykernel_1378/3203550434.py\", line 24, in tokenize\n    for start_idx, end_idx in tokenized.offset_mapping:\nValueError: too many values to unpack (expected 2)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING: \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Configuration class containing various model and training parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(all_labels, label2id, id2label)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, train_df, valid_df)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_df, valid_df):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Create training dataset\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     training_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Create valid dataset\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     valid_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_dataset(valid_df)\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mModelTrainer.create_dataset\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     39\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[1;32m     40\u001b[0m     {\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     }\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel2id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2068\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   2064\u001b[0m             \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m nb_of_missing_shards\n\u001b[1;32m   2065\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre running\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2067\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, async_result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2068\u001b[0m             transformed_shards[index] \u001b[38;5;241m=\u001b[39m \u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   2071\u001b[0m     transformed_shards\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2072\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2074\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcatenating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/multiprocess/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    521\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[38;5;241m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2320\u001b[0m, in \u001b[0;36m_map_single\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2320\u001b[0m         example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   2322\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2220\u001b[0m, in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   2219\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 2220\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2223\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1915\u001b[0m, in \u001b[0;36mdecorated\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1911\u001b[0m decorated_item \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1912\u001b[0m     Example(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m Batch(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m   1913\u001b[0m )\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m result \u001b[38;5;241m=\u001b[39m f(decorated_item, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, LazyDict) \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Labels\u001b[39;00m\n\u001b[1;32m     23\u001b[0m token_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx, end_idx \u001b[38;5;129;01min\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39moffset_mapping:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Added 'O'\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m end_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m         token_labels\u001b[38;5;241m.\u001b[39mappend(label2id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training requires the GPUs and internet\n",
    "TRAINING = True # True: Model Training, False: Model Inference\n",
    "if TRAINING: \n",
    "    # Configuration class containing various model and training parameters\n",
    "    trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "    trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIDE=384\n",
    "MAX_LENGTH = 1024\n",
    "model_path = f\"/kaggle/input/deberta3base-truncation-false-{EXP}-{VER}-small\"\n",
    "threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    tokens = []\n",
    "    token_map = []\n",
    "    idx = 0\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        tokens.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            tokens.append(\" \")\n",
    "            token_map.append(-1) \n",
    "        idx += 1\n",
    "    # Does not truncate the text and concate all words together \n",
    "    # Do not need to have extra space as we have already include it in the previous tokenization\n",
    "    tokenized = tokenizer(\"\".join(tokens), return_offsets_mapping=True, truncation=False)\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from datasets import Dataset \n",
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.model_path = model_path\n",
    "        self.max_length = 1024\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.num_proc = 3 # 3 processors\n",
    "        self.threshold = threshold # Threashold\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)        \n",
    "        # # Load the fine-tuned adapter layer on top of base model\n",
    "        # self.model = self.model.to(DEVICE)\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    def post_processing_preds(self, preds):\n",
    "        preds_final = []\n",
    "        preds_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0],\n",
    "                                                                              preds.shape[1],\n",
    "                                                                              1)\n",
    "        # Get the maximal value as the final preds\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_without_O = preds_softmax[:,:,:12].argmax(-1) # Prob of entity labels (like 'NAME_STUDENT')\n",
    "        O_preds = preds_softmax[:,:,12] # Prob for 'O'\n",
    "        print()\n",
    "        # If preds for 'O' > 0.99, select preds of 'O'. Otherwise, select preds for entity labels.  \n",
    "        preds_final = np.where(O_preds < self.threshold, preds_without_O, preds)\n",
    "        return preds_final     \n",
    "    \n",
    "    def infer_preds(self, ds):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = self.post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer(all_labels, label2id, id2label)\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        try:\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "             # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "    \n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
