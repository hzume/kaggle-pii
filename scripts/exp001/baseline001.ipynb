{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "\n",
    "# Transformer \n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments,\n",
    "                          AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          DebertaV2Config, DebertaV2ForTokenClassification)\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from seqeval.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    more_data = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\")\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "    pii_dataset_fixed = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\")\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    # df = train_data\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "    return df, all_labels, label2id, id2label\n",
    "\n",
    "\n",
    "# Eencode labels to columns\n",
    "def encode_labels(df: pd.DataFrame):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle train data = 6807\n",
      "more data = 2000\n",
      "pii_dataset_fixed = 4434\n",
      "all_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n",
      "EMAIL: 5661  (42.8%)\n",
      "ID_NUM: 1865  (14.1%)\n",
      "NAME_STUDENT: 6834  (51.6%)\n",
      "PHONE_NUM: 4214  (31.8%)\n",
      "STREET_ADDRESS: 5116  (38.6%)\n",
      "URL_PERSONAL: 2546  (19.2%)\n",
      "USERNAME: 2513  (19.0%)\n",
      "others: 5872  (44.3%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>unique_labels</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>NAME_STUDENT</th>\n",
       "      <th>PHONE_NUM</th>\n",
       "      <th>STREET_ADDRESS</th>\n",
       "      <th>URL_PERSONAL</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>13231</td>\n",
       "      <td>My name is Gaspard Ma. I'm a geologist at 0891...</td>\n",
       "      <td>[My, name, is, Gaspard, Ma, ., I, 'm, a, geolo...</td>\n",
       "      <td>[True, True, True, True, False, True, False, T...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13232</th>\n",
       "      <td>13232</td>\n",
       "      <td>In my role as an economist with three years of...</td>\n",
       "      <td>[In, my, role, as, an, economist, with, three,...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13233</th>\n",
       "      <td>13233</td>\n",
       "      <td>My name is Masami Roux. The year was 2019, and...</td>\n",
       "      <td>[My, name, is, Masami, Roux, ., The, year, was...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Tr...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[NAME_STUDENT, PHONE_NUM, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13234</th>\n",
       "      <td>13234</td>\n",
       "      <td>I'm Dr. Kenji Koch, an ophthalmologist with ov...</td>\n",
       "      <td>[I, 'm, Dr., Kenji, Koch, ,, an, ophthalmologi...</td>\n",
       "      <td>[False, True, True, True, False, True, True, T...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[NAME_STUDENT, USERNAME, STREET_ADDRESS, URL_P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>13235</td>\n",
       "      <td>My name is Oleg Richter, and I'm a radiologist...</td>\n",
       "      <td>[My, name, is, Oleg, Richter, ,, and, I, 'm, a...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Fa...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, PHONE_NUM]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>13236</td>\n",
       "      <td>During my 15-year career as a developer, I hav...</td>\n",
       "      <td>[During, my, 15, -, year, career, as, a, devel...</td>\n",
       "      <td>[True, True, False, False, True, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>13237</td>\n",
       "      <td>In 2019, a particularly challenging case came ...</td>\n",
       "      <td>[In, 2019, ,, a, particularly, challenging, ca...</td>\n",
       "      <td>[True, False, True, True, True, True, True, Tr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[STREET_ADDRESS]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>13238</td>\n",
       "      <td>Hello there, I'm Krishna Lopez and I've been a...</td>\n",
       "      <td>[Hello, there, ,, I, 'm, Krishna, Lopez, and, ...</td>\n",
       "      <td>[True, False, True, False, True, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "      <td>[NAME_STUDENT, USERNAME, STREET_ADDRESS, URL_P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>13239</td>\n",
       "      <td>As a designer, I've encountered various challe...</td>\n",
       "      <td>[As, a, designer, ,, I, 've, encountered, vari...</td>\n",
       "      <td>[True, True, False, True, False, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13240</th>\n",
       "      <td>13240</td>\n",
       "      <td>Hello, my name is Hiroko Yu and I am a freelan...</td>\n",
       "      <td>[Hello, ,, my, name, is, Hiroko, Yu, and, I, a...</td>\n",
       "      <td>[False, True, True, True, True, True, True, Tr...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "      <td>[NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       document                                          full_text  \\\n",
       "13231     13231  My name is Gaspard Ma. I'm a geologist at 0891...   \n",
       "13232     13232  In my role as an economist with three years of...   \n",
       "13233     13233  My name is Masami Roux. The year was 2019, and...   \n",
       "13234     13234  I'm Dr. Kenji Koch, an ophthalmologist with ov...   \n",
       "13235     13235  My name is Oleg Richter, and I'm a radiologist...   \n",
       "13236     13236  During my 15-year career as a developer, I hav...   \n",
       "13237     13237  In 2019, a particularly challenging case came ...   \n",
       "13238     13238  Hello there, I'm Krishna Lopez and I've been a...   \n",
       "13239     13239  As a designer, I've encountered various challe...   \n",
       "13240     13240  Hello, my name is Hiroko Yu and I am a freelan...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "13231  [My, name, is, Gaspard, Ma, ., I, 'm, a, geolo...   \n",
       "13232  [In, my, role, as, an, economist, with, three,...   \n",
       "13233  [My, name, is, Masami, Roux, ., The, year, was...   \n",
       "13234  [I, 'm, Dr., Kenji, Koch, ,, an, ophthalmologi...   \n",
       "13235  [My, name, is, Oleg, Richter, ,, and, I, 'm, a...   \n",
       "13236  [During, my, 15, -, year, career, as, a, devel...   \n",
       "13237  [In, 2019, ,, a, particularly, challenging, ca...   \n",
       "13238  [Hello, there, ,, I, 'm, Krishna, Lopez, and, ...   \n",
       "13239  [As, a, designer, ,, I, 've, encountered, vari...   \n",
       "13240  [Hello, ,, my, name, is, Hiroko, Yu, and, I, a...   \n",
       "\n",
       "                                     trailing_whitespace  \\\n",
       "13231  [True, True, True, True, False, True, False, T...   \n",
       "13232  [True, True, True, True, True, True, True, Tru...   \n",
       "13233  [True, True, True, True, False, True, True, Tr...   \n",
       "13234  [False, True, True, True, False, True, True, T...   \n",
       "13235  [True, True, True, True, False, True, True, Fa...   \n",
       "13236  [True, True, False, False, True, True, True, T...   \n",
       "13237  [True, False, True, True, True, True, True, Tr...   \n",
       "13238  [True, False, True, False, True, True, True, T...   \n",
       "13239  [True, True, False, True, False, True, True, T...   \n",
       "13240  [False, True, True, True, True, True, True, Tr...   \n",
       "\n",
       "                                                  labels  \\\n",
       "13231  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13232  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13233  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13234  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13235  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "13236  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13237  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13238  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...   \n",
       "13239  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "13240  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...   \n",
       "\n",
       "                                           unique_labels  EMAIL  ID_NUM  \\\n",
       "13231              [NAME_STUDENT, STREET_ADDRESS, EMAIL]      1       0   \n",
       "13232   [NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]      1       0   \n",
       "13233                   [NAME_STUDENT, PHONE_NUM, EMAIL]      1       0   \n",
       "13234  [NAME_STUDENT, USERNAME, STREET_ADDRESS, URL_P...      0       0   \n",
       "13235          [NAME_STUDENT, STREET_ADDRESS, PHONE_NUM]      0       0   \n",
       "13236   [NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]      1       0   \n",
       "13237                                   [STREET_ADDRESS]      0       0   \n",
       "13238  [NAME_STUDENT, USERNAME, STREET_ADDRESS, URL_P...      0       0   \n",
       "13239              [NAME_STUDENT, STREET_ADDRESS, EMAIL]      1       0   \n",
       "13240   [NAME_STUDENT, STREET_ADDRESS, PHONE_NUM, EMAIL]      1       0   \n",
       "\n",
       "       NAME_STUDENT  PHONE_NUM  STREET_ADDRESS  URL_PERSONAL  USERNAME  others  \n",
       "13231             1          0               1             0         0       0  \n",
       "13232             1          1               1             0         0       0  \n",
       "13233             1          1               0             0         0       0  \n",
       "13234             1          0               1             1         1       0  \n",
       "13235             1          1               1             0         0       0  \n",
       "13236             1          1               1             0         0       0  \n",
       "13237             0          0               1             0         0       0  \n",
       "13238             1          0               1             1         1       0  \n",
       "13239             1          0               1             0         0       0  \n",
       "13240             1          1               1             0         0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data()\n",
    "df_labels, label_classes = encode_labels(df.copy())\n",
    "df_labels.to_csv(\"df_labels.csv\", encoding=\"utf-8\")\n",
    "display(df_labels.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df: pd.DataFrame, n_samples: int):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df[\"document\"].isin(samples_df[\"document\"])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "\n",
    "def downsample_df(df: pd.DataFrame):\n",
    "    \"\"\"Split the df into training and valid dataset\"\"\"\n",
    "    df[\"is_labels\"] = df[\"labels\"].apply(\n",
    "        lambda labels: any(label != \"O\" for label in labels)\n",
    "    )\n",
    "\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df[\"is_labels\"]]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[~df[\"is_labels\"]]\n",
    "\n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples = len(true_labels) - 150\n",
    "\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples = len(false_labels) - 150\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])\n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7219 true_others = 150\n",
      "false_samples = 5722 false_others = 150\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    }
   ],
   "source": [
    "# Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "train_df, valid_df = downsample_df(df.copy())\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(example, tokenizer, label2id):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"provided_labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"]) \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            label_id = label2id[labels[start_idx]]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "    \n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_preds)\n",
    "        precision = precision_score(true_labels, true_preds)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.num_proc = 3\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3  # Number of epochs\n",
    "        self.batch_size = 1  # Too large batch sizes lead to OOM\n",
    "        self.fp16 = True if torch.cuda.is_available() else False\n",
    "        self.model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\"\n",
    "        self.output_dir = \"/kaggle/output\"\n",
    "        self.save_path = \"/kaggle/deberta3base_truncation_false\"\n",
    "        self.load_model()\n",
    "\n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        # Load tokenizer config\n",
    "        config = DebertaV2Config.from_pretrained(self.model_path)\n",
    "        # Increase context length using the max_position_embeddings parameter\n",
    "        config.update(\n",
    "            {\n",
    "                \"num_labels\": len(self.all_labels),\n",
    "                \"id2label\": self.id2label,\n",
    "                \"label2id\": self.label2id,\n",
    "                \"ignore_mismatched_sizes\": True,\n",
    "            }\n",
    "        )\n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_path, config=config\n",
    "        )\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df: pd.DataFrame):\n",
    "        ds = Dataset.from_dict(\n",
    "            {\n",
    "                \"full_text\": df[\"full_text\"].tolist(),\n",
    "                \"document\": df[\"document\"].astype(\"string\"),\n",
    "                \"tokens\": df[\"tokens\"].tolist(),\n",
    "                \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "                \"provided_labels\": df[\"labels\"].tolist(),\n",
    "            }\n",
    "        )\n",
    "        # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(\n",
    "            tokenize,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"label2id\": self.label2id},\n",
    "            num_proc=self.num_proc,\n",
    "        )\n",
    "        return tokenized_ds\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df):\n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(\n",
    "            self.tokenizer, pad_to_multiple_of=16\n",
    "        )\n",
    "\n",
    "        # Trainer cofiguration\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            fp16=self.fp16,  # # Change to False if using CPU only\n",
    "            learning_rate=self.learning_rate,\n",
    "            num_train_epochs=self.num_train_epochs,  # The total number of training epochs to run.\n",
    "            per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "            per_device_eval_batch_size=self.batch_size,  # batch size for evaluation\n",
    "            gradient_accumulation_steps=2,\n",
    "            report_to=\"none\",\n",
    "            evaluation_strategy=\"epoch\",  # Evaluated at the end of epochs\n",
    "            # eval_steps=1,\n",
    "            do_eval=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,  # Save the best and most recent checkpoints\n",
    "            logging_steps=20,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,  # Load the best model at the end\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=0.1,  # number of warmup steps (0.1) for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "        )\n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=training_ds,\n",
    "            eval_dataset=valid_ds,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "        )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training requires the GPUs and internet\n",
    "TRAINING = False # True: Model Training, False: Model Inference\n",
    "if TRAINING: \n",
    "    # Configuration class containing various model and training parameters\n",
    "    trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "    trainer.train(train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIDE=384\n",
    "MAX_LENGTH = 1024\n",
    "model_path = \"/kaggle/input/deberta3base-truncation-false-001-001\"\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    tokens = []\n",
    "    token_map = []\n",
    "    idx = 0\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        tokens.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            tokens.append(\" \")\n",
    "            token_map.append(-1) \n",
    "        idx += 1\n",
    "    # Does not truncate the text and concate all words together \n",
    "    # Do not need to have extra space as we have already include it in the previous tokenization\n",
    "    tokenized = tokenizer(\"\".join(tokens), return_offsets_mapping=True, truncation=False)\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from datasets import Dataset \n",
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.model_path = model_path\n",
    "        self.max_length = 1024\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.num_proc = 3 # 3 processors\n",
    "        self.threshold = threshold # Threashold\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)        \n",
    "        # # Load the fine-tuned adapter layer on top of base model\n",
    "        # self.model = self.model.to(DEVICE)\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "\n",
    "    def post_processing_preds(self, preds):\n",
    "        preds_final = []\n",
    "        preds_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0],\n",
    "                                                                              preds.shape[1],\n",
    "                                                                              1)\n",
    "        # Get the maximal value as the final preds\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_without_O = preds_softmax[:,:,:12].argmax(-1) # Prob of entity labels (like 'NAME_STUDENT')\n",
    "        O_preds = preds_softmax[:,:,12] # Prob for 'O'\n",
    "        print()\n",
    "        # If preds for 'O' > 0.99, select preds of 'O'. Otherwise, select preds for entity labels.  \n",
    "        preds_final = np.where(O_preds < self.threshold, preds_without_O, preds)\n",
    "        return preds_final     \n",
    "    \n",
    "    def infer_preds(self, ds):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = self.post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test dataset 10\n",
      "Complete loading pretrained LLM model\n",
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111d98c0242d419ca61b0ba2377ed6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c50c05412c4f92964c8df6b889ae05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer(all_labels, label2id, id2label)\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        try:\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "             # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "    \n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "\n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1435/1374685706.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_id'] = list(range(len(df)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>479</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>480</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>738</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>739</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>465</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>328</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>330</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>104</td>\n",
       "      <td>9</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>112</td>\n",
       "      <td>28</td>\n",
       "      <td>B-STREET_ADDRESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>112</td>\n",
       "      <td>30</td>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>123</td>\n",
       "      <td>1500</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>123</td>\n",
       "      <td>1509</td>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>123</td>\n",
       "      <td>1575</td>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>123</td>\n",
       "      <td>1575</td>\n",
       "      <td>B-ID_NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>123</td>\n",
       "      <td>1581</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>123</td>\n",
       "      <td>1591</td>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>123</td>\n",
       "      <td>1648</td>\n",
       "      <td>B-URL_PERSONAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_id  document  token             label\n",
       "0        0         7      6    B-NAME_STUDENT\n",
       "1        1         7      7    I-NAME_STUDENT\n",
       "2        2         7      9    B-NAME_STUDENT\n",
       "3        3         7     10    I-NAME_STUDENT\n",
       "4        4         7    479    B-NAME_STUDENT\n",
       "5        5         7    480    I-NAME_STUDENT\n",
       "6        6         7    482    B-NAME_STUDENT\n",
       "7        7         7    483    I-NAME_STUDENT\n",
       "8        8         7    738    B-NAME_STUDENT\n",
       "9        9         7    739    I-NAME_STUDENT\n",
       "10      10         7    741    B-NAME_STUDENT\n",
       "11      11         7    742    I-NAME_STUDENT\n",
       "12      12        10      0    B-NAME_STUDENT\n",
       "13      13        10      1    I-NAME_STUDENT\n",
       "14      14        10    464    B-NAME_STUDENT\n",
       "15      15        10    465    I-NAME_STUDENT\n",
       "16      16        16      4    B-NAME_STUDENT\n",
       "17      17        16      5    I-NAME_STUDENT\n",
       "18      18        20      5    B-NAME_STUDENT\n",
       "19      19        20      6    I-NAME_STUDENT\n",
       "20      20        20      8    I-NAME_STUDENT\n",
       "21      21        20    328    B-NAME_STUDENT\n",
       "22      22        20    330    B-NAME_STUDENT\n",
       "23      23        56     12    B-NAME_STUDENT\n",
       "24      24        56     13    I-NAME_STUDENT\n",
       "25      25        86      6    B-NAME_STUDENT\n",
       "26      26        86      7    I-NAME_STUDENT\n",
       "27      27        93      0    B-NAME_STUDENT\n",
       "28      28        93      1    I-NAME_STUDENT\n",
       "29      29       104      7    B-NAME_STUDENT\n",
       "30      30       104      8    B-NAME_STUDENT\n",
       "31      31       104      9    I-NAME_STUDENT\n",
       "32      32       112      5    B-NAME_STUDENT\n",
       "33      33       112      6    I-NAME_STUDENT\n",
       "34      34       112     28  B-STREET_ADDRESS\n",
       "35      35       112     30  I-STREET_ADDRESS\n",
       "36      36       123     32    B-NAME_STUDENT\n",
       "37      37       123     33    I-NAME_STUDENT\n",
       "38      38       123   1500    B-NAME_STUDENT\n",
       "39      39       123   1509    B-URL_PERSONAL\n",
       "40      40       123   1575    B-URL_PERSONAL\n",
       "41      41       123   1575          B-ID_NUM\n",
       "42      42       123   1581    B-NAME_STUDENT\n",
       "43      43       123   1591    B-URL_PERSONAL\n",
       "44      44       123   1648    B-URL_PERSONAL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
